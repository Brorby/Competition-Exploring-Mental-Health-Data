{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import dynamic_selection as ds\n",
    "from dynamic_selection import MaskingPretrainer, GreedyDynamicSelection\n",
    "from torch.distributions import Distribution\n",
    "Distribution.set_default_validate_args(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# Python\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# NumPy\n",
    "np.random.seed(seed)\n",
    "\n",
    "# PyTorch\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "# For deterministic behavior on GPU (may slow training)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "Here we explore each feature column and comment on how we are going to preprocess each one and what we might consider doing in the future. We have a total of 140700 samples. The last feature is the label depression which we will train on.\n",
    "\n",
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples in training set 140700\n",
      "Samples in test set: 93800\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "data_file = 'data/train.csv'\n",
    "test_file = 'data/test.csv'\n",
    "\n",
    "data = pd.read_csv(data_file)\n",
    "test = pd.read_csv(test_file)\n",
    "\n",
    "print(f\"Samples in training set {data.shape[0]}\")\n",
    "print(f\"Samples in test set: {test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# id Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of 'id' column:\n",
      "0    0\n",
      "1    1\n",
      "2    2\n",
      "3    3\n",
      "4    4\n",
      "Name: id, dtype: int64\n",
      "\n",
      "We have  140700  amount of samples\n"
     ]
    }
   ],
   "source": [
    "# Print examples of the 'id' column\n",
    "print(\"Examples of 'id' column:\")\n",
    "print(data[\"id\"].head())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"We have \" ,data[\"id\"].count(),\" amount of samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop the id column since there is no correlation between this and the label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique names sorted by count (most to least):\n",
      "Rohan: 3178\n",
      "Aarav: 2336\n",
      "Rupak: 2176\n",
      "Aaradhya: 2045\n",
      "Anvi: 2035\n",
      "Raghavendra: 1877\n",
      "Vani: 1657\n",
      "Tushar: 1596\n",
      "Ritvik: 1589\n",
      "Shiv: 1568\n",
      "Riya: 1548\n",
      "Rashi: 1547\n",
      "Raunak: 1524\n",
      "Anand: 1486\n",
      "Ishaani: 1477\n",
      "Ansh: 1423\n",
      "Vidya: 1408\n",
      "Ritika: 1313\n",
      "Anushka: 1279\n",
      "Sanya: 1272\n",
      "Aarush: 1266\n",
      "Aariv: 1254\n",
      "Abhishek: 1252\n",
      "Rupal: 1234\n",
      "Harsha: 1230\n",
      "Harsh: 1156\n",
      "Vikram: 1154\n",
      "Shivam: 1146\n",
      "Raghav: 1120\n",
      "Armaan: 1116\n",
      "Prachi: 1104\n",
      "Ayaan: 1090\n",
      "Ivaan: 1090\n",
      "Siddhesh: 1090\n",
      "Ira: 1061\n",
      "Prisha: 1055\n",
      "Rahil: 1051\n",
      "Rishi: 1040\n",
      "Ritik: 1033\n",
      "Aniket: 1023\n",
      "Pratham: 1023\n",
      "Chhavi: 1003\n",
      "Vibha: 974\n",
      "Vivan: 963\n",
      "Aishwarya: 962\n",
      "Gauri: 959\n",
      "Nikita: 951\n",
      "Naina: 946\n",
      "Veda: 940\n",
      "Arav: 925\n",
      "Vidhi: 913\n",
      "Jiya: 912\n",
      "Advait: 910\n",
      "Krishna: 875\n",
      "Vedant: 872\n",
      "Ayush: 869\n",
      "Aditi: 851\n",
      "Shaurya: 848\n",
      "Kashish: 845\n",
      "Gagan: 842\n",
      "Eshita: 831\n",
      "Pratyush: 816\n",
      "Ila: 799\n",
      "Simran: 790\n",
      "Aadhya: 787\n",
      "Shreya: 784\n",
      "Rudransh: 776\n",
      "Garima: 769\n",
      "Yashvi: 767\n",
      "Anjali: 757\n",
      "Vihaan: 744\n",
      "Keshav: 737\n",
      "Yuvraj: 730\n",
      "Ishan: 714\n",
      "Tanisha: 713\n",
      "Harshil: 712\n",
      "Sanket: 710\n",
      "Rajat: 706\n",
      "Kunal: 700\n",
      "Nikhil: 690\n",
      "Kiran: 687\n",
      "Aanchal: 684\n",
      "Zara: 681\n",
      "Shlok: 672\n",
      "Nandini: 663\n",
      "Pranav: 655\n",
      "Mahika: 652\n",
      "Kavya: 643\n",
      "Jhanvi: 635\n",
      "Satyam: 632\n",
      "Esha: 632\n",
      "Reyansh: 627\n",
      "Shrey: 626\n",
      "Janvi: 611\n",
      "Anaya: 610\n",
      "Anirudh: 606\n",
      "Arnav: 595\n",
      "Aahana: 593\n",
      "Aarti: 586\n",
      "Pallavi: 586\n",
      "Amit: 583\n",
      "Ishwar: 574\n",
      "Utkarsh: 547\n",
      "Siddharth: 539\n",
      "Saanvi: 537\n",
      "Asha: 534\n",
      "Darsh: 526\n",
      "Abhinav: 524\n",
      "Anika: 520\n",
      "Parth: 518\n",
      "Aditya: 512\n",
      "Aarohi: 504\n",
      "Tanya: 501\n",
      "Nishant: 501\n",
      "Rhea: 499\n",
      "Samar: 498\n",
      "Chirag: 493\n",
      "Kriti: 490\n",
      "Manvi: 486\n",
      "Divya: 481\n",
      "Kabir: 481\n",
      "Kian: 481\n",
      "Pooja: 481\n",
      "Hrithik: 475\n",
      "Shruti: 467\n",
      "Ayansh: 461\n",
      "Mahi: 461\n",
      "Trisha: 458\n",
      "Kartikeya: 456\n",
      "Atharv: 438\n",
      "Mihir: 433\n",
      "Monika: 433\n",
      "Shivansh: 419\n",
      "Tanmay: 418\n",
      "Arjun: 403\n",
      "Diya: 391\n",
      "Yash: 389\n",
      "Charvi: 387\n",
      "Sanjeev: 387\n",
      "Vrinda: 383\n",
      "Sai: 382\n",
      "Aryan: 379\n",
      "Manan: 378\n",
      "Ishita: 377\n",
      "Damini: 374\n",
      "Virat: 374\n",
      "Bhavesh: 362\n",
      "Khushi: 356\n",
      "Ishaan: 355\n",
      "Nalini: 353\n",
      "Jai: 343\n",
      "Neha: 342\n",
      "Palak: 339\n",
      "Yogesh: 338\n",
      "Gaurav: 336\n",
      "Deepak: 335\n",
      "Dhruv: 334\n",
      "Tara: 331\n",
      "Vivaan: 329\n",
      "Pari: 319\n",
      "Lavanya: 315\n",
      "Aakash: 313\n",
      "Navya: 311\n",
      "Leela: 310\n",
      "Siddhi: 310\n",
      "Soham: 306\n",
      "Tanvi: 300\n",
      "Kartik: 296\n",
      "Arya: 289\n",
      "Tina: 285\n",
      "Vanya: 280\n",
      "Kush: 274\n",
      "Isha: 266\n",
      "Nisha: 264\n",
      "Samaira: 263\n",
      "Suhani: 261\n",
      "Saurav: 258\n",
      "Apoorva: 252\n",
      "Yamini: 250\n",
      "Aarya: 247\n",
      "Neil: 244\n",
      "Karishma: 240\n",
      "Lata: 237\n",
      "Zoya: 232\n",
      "Nirvaan: 231\n",
      "Rudra: 220\n",
      "Vaishnavi: 217\n",
      "Om: 216\n",
      "Mayank: 215\n",
      "Dev: 210\n",
      "Vaanya: 209\n",
      "Nakul: 206\n",
      "Rajveer: 199\n",
      "Himani: 196\n",
      "Barkha: 190\n",
      "Tejas: 185\n",
      "Sneha: 180\n",
      "Bhavna: 167\n",
      "Ranveer: 162\n",
      "Rupa: 151\n",
      "Srishti: 150\n",
      "Kiara: 138\n",
      "Kanika: 138\n",
      "Mira: 137\n",
      "Mithila: 132\n",
      "Radhika: 122\n",
      "Sara: 108\n",
      "Pihu: 104\n",
      "Varun: 97\n",
      "Jasmine: 94\n",
      "Lakshay: 80\n",
      "Avni: 71\n",
      "Shanaya: 65\n",
      "Mukund: 57\n",
      "Meera: 51\n",
      "Harini: 47\n",
      "Kalyan: 21\n",
      "Kolkata: 3\n",
      "Aarash: 3\n",
      "Tarsh: 3\n",
      "Rupar: 3\n",
      "Siddh: 2\n",
      "Shivar: 2\n",
      "Aarvi: 2\n",
      "Aisha: 2\n",
      "Rani: 2\n",
      "Aanya: 2\n",
      "Jhaan: 2\n",
      "Patna: 2\n",
      "Aan: 2\n",
      "Pradhya: 2\n",
      "Thane: 2\n",
      "Kartika: 2\n",
      "Rupadhya: 2\n",
      "Aam: 2\n",
      "Hra: 2\n",
      "Shivvi: 2\n",
      "Nanya: 2\n",
      "Prishti: 2\n",
      "Vashi: 2\n",
      "Aniv: 2\n",
      "Shivan: 2\n",
      "Shivna: 2\n",
      "Anya: 2\n",
      "Niki: 1\n",
      "Shivsh: 1\n",
      "Ishaesh: 1\n",
      "Prishi: 1\n",
      "Rohik: 1\n",
      "Nya: 1\n",
      "Harshav: 1\n",
      "Ewesh: 1\n",
      "Aanket: 1\n",
      "Tohar: 1\n",
      "Ryouvik: 1\n",
      "Niya: 1\n",
      "Anjun: 1\n",
      "Rupat: 1\n",
      "Anahk: 1\n",
      "Kanisha: 1\n",
      "Anisha: 1\n",
      "Harshaun: 1\n",
      "Abishma: 1\n",
      "Mahak: 1\n",
      "Parvik: 1\n",
      "Vlaan: 1\n",
      "Krav: 1\n",
      "A.Ed: 1\n",
      "Ayhan: 1\n",
      "Prvi: 1\n",
      "Parvi: 1\n",
      "Rudegrav: 1\n",
      "Kupa: 1\n",
      "Aarsh: 1\n",
      "Rupil: 1\n",
      "Ani: 1\n",
      "Aieter: 1\n",
      "Nishita: 1\n",
      "Irit: 1\n",
      "Aarani: 1\n",
      "Anariv: 1\n",
      "Ritak: 1\n",
      "Kashi: 1\n",
      "Kike: 1\n",
      "Shivvaan: 1\n",
      "Aarla: 1\n",
      "Nishi: 1\n",
      "Aarand: 1\n",
      "Adiya: 1\n",
      "Shivak: 1\n",
      "Golkut: 1\n",
      "Kartal: 1\n",
      "Krey: 1\n",
      "Prarav: 1\n",
      "Aiya: 1\n",
      "Anil: 1\n",
      "Ayash: 1\n",
      "Vivani: 1\n",
      "R.Com: 1\n",
      "Anohi: 1\n",
      "Rietal: 1\n",
      "18: 1\n",
      "Aavya: 1\n",
      "Ronnie: 1\n",
      "Viv: 1\n",
      "Anarush: 1\n",
      "BE: 1\n",
      "Raghavvi: 1\n",
      "Rajankot: 1\n",
      "Taurav: 1\n",
      "K. Kavya: 1\n",
      "Rai: 1\n",
      "Ayut: 1\n",
      "Pariv: 1\n",
      "Gavrachi: 1\n",
      "Hreya: 1\n",
      "Shivivaam: 1\n",
      "M.Com: 1\n",
      "Noreen: 1\n",
      "Haurav: 1\n",
      "Rivaan: 1\n",
      "Anakash: 1\n",
      "Jush: 1\n",
      "Randik: 1\n",
      "Adachi: 1\n",
      "Tinmay: 1\n",
      "Vohi: 1\n",
      "Aarat: 1\n",
      "Aariket: 1\n",
      "Aohi: 1\n",
      "Anish: 1\n",
      "Zahra: 1\n",
      "Rieta: 1\n",
      "Vavya: 1\n",
      "Jhav: 1\n",
      "Jathesh: 1\n",
      "Anh: 1\n",
      "Vidvi: 1\n",
      "Mahir: 1\n",
      "Raghavik: 1\n",
      "Ishaam: 1\n",
      "Anhil: 1\n",
      "Prishant: 1\n",
      "Sansh: 1\n",
      "Researcher: 1\n",
      "Eirini: 1\n",
      "Shaina: 1\n",
      "Aani: 1\n",
      "Plumber: 1\n",
      "Tanak: 1\n",
      "Manik: 1\n",
      "Nanchal: 1\n",
      "Ayoub: 1\n",
      "Siddir: 1\n",
      "Nikya: 1\n",
      "Arnar: 1\n",
      "Vakash: 1\n",
      "Rhesh: 1\n",
      "Kani: 1\n",
      "Vidra: 1\n",
      "Aarsush: 1\n",
      "Zegmay: 1\n",
      "Airav: 1\n",
      "Naly: 1\n",
      "Chrinda: 1\n",
      "Ishma: 1\n",
      "Vita: 1\n",
      "Prandini: 1\n",
      "Mahav: 1\n",
      "Nhanini: 1\n",
      "Anupal: 1\n",
      "Tani: 1\n",
      "Jaish: 1\n",
      "Rajya: 1\n",
      "Arsha: 1\n",
      "Eikram: 1\n",
      "Manr: 1\n",
      "Harshir: 1\n",
      "Shivwar: 1\n",
      "Abarav: 1\n",
      "Ayya: 1\n",
      "Shivlok: 1\n",
      "Rupai: 1\n",
      "Rudrithik: 1\n",
      "K.Pharm: 1\n",
      "Varanasi: 1\n",
      "UX/UI Designer: 1\n",
      "Ariti: 1\n",
      "Harshand: 1\n",
      "Ranchal: 1\n",
      "Arvik: 1\n",
      "Manjun: 1\n",
      "Ijra: 1\n",
      "Rupika: 1\n",
      "Parha: 1\n",
      "Vidha: 1\n",
      "Prayat: 1\n",
      "Aaransh: 1\n",
      "Yurav: 1\n",
      "Srinagar: 1\n",
      "Aaranya: 1\n",
      "Anishi: 1\n",
      "Aikash: 1\n",
      "Nhanvi: 1\n",
      "Jiram: 1\n",
      "Anar: 1\n",
      "Rudrey: 1\n",
      "Shashi: 1\n",
      "Eisha: 1\n",
      "Vhaani: 1\n",
      "Prilak: 1\n",
      "Ishaansh: 1\n",
      "Virar: 1\n",
      "Sharth: 1\n",
      "M.Tech: 1\n",
      "Shir: 1\n",
      "Rietvik: 1\n",
      "Vasai-Virar: 1\n",
      "Ishlok: 1\n",
      "Vika: 1\n",
      "Rika: 1\n",
      "Aarun: 1\n",
      "Total unique names: 422\n"
     ]
    }
   ],
   "source": [
    "# Count occurrences of each unique name\n",
    "name_counts = data[\"Name\"].value_counts()\n",
    "\n",
    "# Print results\n",
    "print(\"Unique names sorted by count (most to least):\")\n",
    "for name, count in name_counts.items():\n",
    "    print(f\"{name}: {count}\")\n",
    "\n",
    "print(f\"Total unique names: {len(name_counts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thought about dropping the name column since we thought that this may not have any correlation to the label. But we realised that there might be a correlation, so we decided to keep it. The correlation being that for example having a \"unattractive\" name can affect your life in a bad way. \n",
    "\n",
    "\n",
    "For pre-processing this we will to give a value of how unique it is in the dataset, so for now we change the name with how frequent it is.\n",
    "\n",
    "\n",
    "Things to consider:\n",
    "\n",
    "Find a way to \"rate\" each name instead of how frequent the name is. Impute or change wrong names to missing name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of 'Name' column:\n",
      "0    Female\n",
      "1      Male\n",
      "2      Male\n",
      "3      Male\n",
      "4    Female\n",
      "Name: Gender, dtype: object\n",
      "\n",
      "Number of unique genders: 2\n"
     ]
    }
   ],
   "source": [
    "# Print examples of the 'Name' column\n",
    "print(\"Examples of 'Name' column:\")\n",
    "print(data[\"Gender\"].head())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of unique genders:\", data[\"Gender\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just encode male to 1 and female to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of 'Age' column:\n",
      "0    49.0\n",
      "1    26.0\n",
      "2    33.0\n",
      "3    22.0\n",
      "4    30.0\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "Number of unique ages: 43\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples of 'Age' column:\")\n",
    "print(data[\"Age\"].head())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of unique ages:\", data[\"Age\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only change the number from float to integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of 'City' column:\n",
      "0         Ludhiana\n",
      "1         Varanasi\n",
      "2    Visakhapatnam\n",
      "3           Mumbai\n",
      "4           Kanpur\n",
      "Name: City, dtype: object\n",
      "\n",
      "Number of unique cities: 43\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples of 'City' column:\")\n",
    "print(data[\"City\"].head())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of unique cities:\", data[\"Age\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique cities sorted by count (most to least):\n",
      "Kalyan: First Index = 36, Count = 6591\n",
      "Patna: First Index = 9, Count = 5924\n",
      "Vasai-Virar: First Index = 49, Count = 5765\n",
      "Kolkata: First Index = 28, Count = 5689\n",
      "Ahmedabad: First Index = 5, Count = 5613\n",
      "Meerut: First Index = 17, Count = 5528\n",
      "Ludhiana: First Index = 0, Count = 5226\n",
      "Pune: First Index = 13, Count = 5210\n",
      "Rajkot: First Index = 10, Count = 5207\n",
      "Visakhapatnam: First Index = 2, Count = 5176\n",
      "Srinagar: First Index = 26, Count = 5074\n",
      "Mumbai: First Index = 3, Count = 4966\n",
      "Indore: First Index = 189, Count = 4872\n",
      "Agra: First Index = 18, Count = 4684\n",
      "Surat: First Index = 20, Count = 4636\n",
      "Varanasi: First Index = 1, Count = 4606\n",
      "Vadodara: First Index = 43, Count = 4568\n",
      "Hyderabad: First Index = 23, Count = 4496\n",
      "Kanpur: First Index = 4, Count = 4398\n",
      "Jaipur: First Index = 12, Count = 4328\n",
      "Thane: First Index = 6, Count = 4289\n",
      "Lucknow: First Index = 16, Count = 4280\n",
      "Nagpur: First Index = 37, Count = 4209\n",
      "Bangalore: First Index = 8, Count = 4123\n",
      "Chennai: First Index = 34, Count = 4044\n",
      "Ghaziabad: First Index = 27, Count = 3620\n",
      "Delhi: First Index = 76, Count = 3593\n",
      "Bhopal: First Index = 141, Count = 3475\n",
      "Faridabad: First Index = 21, Count = 3268\n",
      "Nashik: First Index = 7, Count = 3144\n",
      "Mihir: First Index = 44166, Count = 7\n",
      "Nandini: First Index = 32706, Count = 4\n",
      "Harsha: First Index = 15181, Count = 3\n",
      "Saanvi: First Index = 22293, Count = 3\n",
      "Mahi: First Index = 76335, Count = 3\n",
      "Vidya: First Index = 52500, Count = 3\n",
      "Pratyush: First Index = 34247, Count = 3\n",
      "Bhavna: First Index = 28862, Count = 3\n",
      "City: First Index = 34300, Count = 3\n",
      "M.Com: First Index = 33276, Count = 2\n",
      "Nalini: First Index = 17337, Count = 2\n",
      "Ayush: First Index = 2123, Count = 2\n",
      "Atharv: First Index = 33447, Count = 2\n",
      "MCA: First Index = 38232, Count = 2\n",
      "Molkata: First Index = 100610, Count = 2\n",
      "Keshav: First Index = 12371, Count = 2\n",
      "Gurgaon: First Index = 3554, Count = 1\n",
      "Vidhi: First Index = 1190, Count = 1\n",
      "Ishanabad: First Index = 762, Count = 1\n",
      "Raghavendra: First Index = 22215, Count = 1\n",
      "Plata: First Index = 33408, Count = 1\n",
      "Less Delhi: First Index = 30231, Count = 1\n",
      "M.Tech: First Index = 28672, Count = 1\n",
      "Aishwarya: First Index = 8772, Count = 1\n",
      "Krishna: First Index = 3753, Count = 1\n",
      "Mira: First Index = 38817, Count = 1\n",
      "Less than 5 Kalyan: First Index = 36993, Count = 1\n",
      "3.0: First Index = 35309, Count = 1\n",
      "Morena: First Index = 42134, Count = 1\n",
      "Moreadhyay: First Index = 39217, Count = 1\n",
      "Kashk: First Index = 43050, Count = 1\n",
      "Ishkarsh: First Index = 42470, Count = 1\n",
      "Tolkata: First Index = 53366, Count = 1\n",
      "Anvi: First Index = 55970, Count = 1\n",
      "Aditya: First Index = 18496, Count = 1\n",
      "Malyansh: First Index = 20886, Count = 1\n",
      "Ayansh: First Index = 58776, Count = 1\n",
      "Krinda: First Index = 57185, Count = 1\n",
      "Vaanya: First Index = 63830, Count = 1\n",
      "Shrey: First Index = 60210, Count = 1\n",
      "Harsh: First Index = 68667, Count = 1\n",
      "Reyansh: First Index = 70407, Count = 1\n",
      "Kashish: First Index = 71995, Count = 1\n",
      "Ivaan: First Index = 61687, Count = 1\n",
      "Kibara: First Index = 73510, Count = 1\n",
      "Vaishnavi: First Index = 73571, Count = 1\n",
      "Chhavi: First Index = 74881, Count = 1\n",
      "Parth: First Index = 75591, Count = 1\n",
      "Tushar: First Index = 77791, Count = 1\n",
      "MSc: First Index = 82304, Count = 1\n",
      "No: First Index = 82472, Count = 1\n",
      "Gaurav: First Index = 65042, Count = 1\n",
      "Rashi: First Index = 86723, Count = 1\n",
      "ME: First Index = 93066, Count = 1\n",
      "Researcher: First Index = 105684, Count = 1\n",
      "Kagan: First Index = 106809, Count = 1\n",
      "Armaan: First Index = 109183, Count = 1\n",
      "Ithal: First Index = 112975, Count = 1\n",
      "Nalyan: First Index = 115761, Count = 1\n",
      "Dhruv: First Index = 116546, Count = 1\n",
      "Galesabad: First Index = 117760, Count = 1\n",
      "Itheg: First Index = 121908, Count = 1\n",
      "Aaradhya: First Index = 123642, Count = 1\n",
      "Pooja: First Index = 124395, Count = 1\n",
      "Khushi: First Index = 125289, Count = 1\n",
      "Khaziabad: First Index = 126752, Count = 1\n",
      "Jhanvi: First Index = 136563, Count = 1\n",
      "Unirar: First Index = 138190, Count = 1\n",
      "Total unique cities: 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elias\\AppData\\Local\\Temp\\ipykernel_43244\\1198885951.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  first_occurrence = data.groupby(\"City\").apply(lambda x: x.index[0])\n"
     ]
    }
   ],
   "source": [
    "# Count occurrences of each unique city\n",
    "city_counts = data[\"City\"].value_counts()\n",
    "\n",
    "# Find first occurrence index for each unique city\n",
    "first_occurrence = data.groupby(\"City\").apply(lambda x: x.index[0])\n",
    "\n",
    "# Sort cities by count in descending order\n",
    "sorted_cities = city_counts.index  # Cities sorted by count (default sorting from most to least)\n",
    "\n",
    "count = 0\n",
    "# Print results\n",
    "print(\"Unique cities sorted by count (most to least):\")\n",
    "for city in sorted_cities:\n",
    "    if city_counts[city] > 0:\n",
    "        count += 1\n",
    "        print(f\"{city}: First Index = {first_occurrence[city]}, Count = {city_counts[city]}\")\n",
    "\n",
    "print(f\"Total unique cities: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found population, density, literacy and sex ratio for each major city in our dataset. We then merged this in our data and removed city column. For the minor cities or the wrongly written cities we took the average of the other columns.\n",
    "\n",
    "\n",
    "To consider:\n",
    "\n",
    "We should consider adjusting the imputing of the minor and wrongly written cities to for example lower than average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Professional or Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of 'Working Professional or Student' column:\n",
      "0    Working Professional\n",
      "1    Working Professional\n",
      "2                 Student\n",
      "3    Working Professional\n",
      "4    Working Professional\n",
      "Name: Working Professional or Student, dtype: object\n",
      "\n",
      "Number of unique Working Professional or Student: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples of 'Working Professional or Student' column:\")\n",
    "print(data[\"Working Professional or Student\"].head())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of unique Working Professional or Student:\", data[\"Working Professional or Student\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this column we will change \"working professional\" to 0 and \"student\" to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of 'Profession' column:\n",
      "0                Chef\n",
      "1             Teacher\n",
      "2                 NaN\n",
      "3             Teacher\n",
      "4    Business Analyst\n",
      "Name: Profession, dtype: object\n",
      "\n",
      "NaN is student:\n",
      "nan\n",
      "Student\n",
      "\n",
      "Number of unique Profession: 64\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples of 'Profession' column:\")\n",
    "print(data[\"Profession\"].head())\n",
    "print(\"\")\n",
    "print(\"NaN is student:\")\n",
    "print(data[\"Profession\"][2])\n",
    "print(data[\"Working Professional or Student\"][2])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of unique Profession:\", data[\"Profession\"].nunique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are occurences of NaN in this column, this happens when the sample is a student. We find it reasonable to insert \"Student\" in those slots. there are also occurences of NaN on samples that are not students, here we will insert \"Missing Profession\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Academic Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of 'Academic pressure' column:\n",
      "0    NaN\n",
      "1    NaN\n",
      "2    5.0\n",
      "3    NaN\n",
      "4    NaN\n",
      "Name: Academic Pressure, dtype: float64\n",
      "\n",
      "Number of unique Academic pressures: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples of 'Academic pressure' column:\")\n",
    "print(data[\"Academic Pressure\"].head())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of unique Academic pressures:\", data[\"Academic Pressure\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the \"Academic Pressure\" column we want to replace NaN with zeros.\n",
    "We are aslo considering merging this feature with work pressure, since they complete eachother. but for now we will have it like this and maybe change it for improving the model later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of 'Work Pressure' column:\n",
      "0    5.0\n",
      "1    4.0\n",
      "2    NaN\n",
      "3    5.0\n",
      "4    1.0\n",
      "Name: Work Pressure, dtype: float64\n",
      "\n",
      "Number of unique Work Pressure: [ 5.  4. nan  1.  2.  3.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples of 'Work Pressure' column:\")\n",
    "print(data[\"Work Pressure\"].head())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of unique Work Pressure:\", data[\"Work Pressure\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that a lower number means less work pressure, therefore We will change Nan to 0 because it is the students that has NaN on Work Pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CGPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of 'CGPA' column:\n",
      "0     NaN\n",
      "1     NaN\n",
      "2    8.97\n",
      "3     NaN\n",
      "4     NaN\n",
      "Name: CGPA, dtype: float64\n",
      "7.658636192558608\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples of 'CGPA' column:\")\n",
    "print(data[\"CGPA\"].head())\n",
    "print(data[\"CGPA\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider changing the NaN slots to the average of the dataset, which is 7.66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study Satisfaction and Job Satisfaction (Satisfaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of 'Study Satisfaction' and 'Job Satisfaction' column:\n",
      "0    NaN\n",
      "1    NaN\n",
      "2    2.0\n",
      "3    NaN\n",
      "4    NaN\n",
      "Name: Study Satisfaction, dtype: float64\n",
      "0    2.0\n",
      "1    3.0\n",
      "2    NaN\n",
      "3    1.0\n",
      "4    1.0\n",
      "Name: Job Satisfaction, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples of 'Study Satisfaction' and 'Job Satisfaction' column:\")\n",
    "print(data[\"Study Satisfaction\"].head())\n",
    "print(data[\"Job Satisfaction\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can tell that when there is missing a value in the study satisfaction column, there is a value in the same sample but on the job satisfaction problem. these two columns complete eachother, so we will combine these two columns into one \"satisfaction\" column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sleep sorted by count (most to least):\n",
      "Less than 5 hours 38784\n",
      "7-8 hours 36969\n",
      "More than 8 hours 32726\n",
      "5-6 hours 32142\n",
      "3-4 hours 12\n",
      "6-7 hours 8\n",
      "4-5 hours 7\n"
     ]
    }
   ],
   "source": [
    "sleep_counts = data[\"Sleep Duration\"].value_counts()\n",
    "\n",
    "print(\"Unique sleep sorted by count (most to least):\")\n",
    "for e, (duration, count) in enumerate(sleep_counts.items()):\n",
    "    print(duration, count)\n",
    "    if e == 6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one looks a bit tricky. since there are so few occurnces of other than the four most common inputs in this feature, we will change the numbers to a scale from 1 to 4, where 1 is \"less than 5 hours\" all the way to 4 which is \"more than 8 hours\". all the others will be set to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dietary Habits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sleep sorted by count (most to least):\n",
      "Moderate 49705\n",
      "Unhealthy 46227\n",
      "Healthy 44741\n",
      "Yes 2\n",
      "More Healthy 2\n",
      "No 2\n",
      "Pratham 1\n"
     ]
    }
   ],
   "source": [
    "diet_counts = data[\"Dietary Habits\"].value_counts()\n",
    "\n",
    "print(\"Unique sleep sorted by count (most to least):\")\n",
    "for e, (diet, count) in enumerate(diet_counts.items()):\n",
    "    print(diet, count)\n",
    "    if e == 6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Healthy -> 2\n",
    "* Moderate -> 1\n",
    "* Unhealthy -> 0\n",
    "* The rest -> 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "Unique sleep sorted by count (most to least):\n",
      "Class 12 14729\n",
      "B.Ed 11691\n",
      "B.Arch 8742\n",
      "B.Com 8113\n",
      "B.Pharm 5856\n",
      "BCA 5739\n",
      "M.Ed 5668\n",
      "MCA 5234\n",
      "BBA 5030\n",
      "BSc 5027\n",
      "MSc 4879\n"
     ]
    }
   ],
   "source": [
    "degree_counts = data[\"Degree\"].value_counts()\n",
    "print(data[\"Degree\"].nunique())\n",
    "\n",
    "print(\"Unique sleep sorted by count (most to least):\")\n",
    "for e, (degree, count) in enumerate(degree_counts.items()):\n",
    "    print(degree, count)\n",
    "    if e == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is particularly difficult because there are 115 unique degrees and there are not just a few degrees that covers the majority of the dataset either as the case is in the dietary habit feature. Our approach here is to somehow categorize the different degrees into bachelor, master, doctrine etc. and then give each of them a number from -1 to 4 based on the rank of the degree, going from \"other\" (which is the case where we can't define what degree it is) to professional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have you ever had suicidal thoughts ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of 'Have you ever had suicidal thoughts ?' column:\n",
      "0     No\n",
      "1    Yes\n",
      "2    Yes\n",
      "3    Yes\n",
      "4    Yes\n",
      "Name: Have you ever had suicidal thoughts ?, dtype: object\n",
      "\n",
      "Number of unique Have you ever had suicidal thoughts ?: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples of 'Have you ever had suicidal thoughts ?' column:\")\n",
    "print(data[\"Have you ever had suicidal thoughts ?\"].head())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of unique Have you ever had suicidal thoughts ?:\", data[\"Have you ever had suicidal thoughts ?\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also change this column to binary no / yes to 0 / 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work Study Hours (No prerocessing needed here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "Unique Work/Study Hours sorted by count (most to least):\n",
      "10.0 14199\n",
      "11.0 12832\n",
      "9.0 12711\n",
      "0.0 12066\n",
      "12.0 11409\n",
      "2.0 10595\n",
      "6.0 10432\n",
      "7.0 9872\n",
      "1.0 9802\n",
      "3.0 9474\n",
      "5.0 9337\n",
      "4.0 9065\n",
      "8.0 8906\n"
     ]
    }
   ],
   "source": [
    "ws_counts = data[\"Work/Study Hours\"].value_counts()\n",
    "print(data[\"Work/Study Hours\"].nunique())\n",
    "\n",
    "print(\"Unique Work/Study Hours sorted by count (most to least):\")\n",
    "for e, (ws, count) in enumerate(ws_counts.items()):\n",
    "    print(ws, count)\n",
    "    if e == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Unique sleep sorted by count (most to least):\n",
      "2.0 31451\n",
      "5.0 28279\n",
      "4.0 27765\n",
      "1.0 27211\n",
      "3.0 25990\n"
     ]
    }
   ],
   "source": [
    "finans_count = data[\"Financial Stress\"].value_counts()\n",
    "print(data[\"Financial Stress\"].nunique())\n",
    "\n",
    "print(\"Unique sleep sorted by count (most to least):\")\n",
    "for e, (fn, count) in enumerate(finans_count.items()):\n",
    "    print(fn, count)\n",
    "    if e == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has a few missing values, we will impute the mean in these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Family History of Mental Illness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of 'Family History of Mental Illness' column:\n",
      "0     No\n",
      "1     No\n",
      "2     No\n",
      "3    Yes\n",
      "4    Yes\n",
      "Name: Family History of Mental Illness, dtype: object\n",
      "\n",
      "Number of unique Family History of Mental Illnesses: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples of 'Family History of Mental Illness' column:\")\n",
    "print(data[\"Family History of Mental Illness\"].head())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of unique Family History of Mental Illnesses:\", data[\"Family History of Mental Illness\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this column we will change yes and no to 1 and 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depression - The label\n",
    "Plotting the distribution of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_counts = data['Depression'].value_counts()\n",
    "target_counts.plot(kind='bar', color=['pink', 'black'])\n",
    "plt.xticks(ticks=[0, 1], labels=['Not Depressed', 'Depressed'], rotation=0)\n",
    "plt.title('Distribution of Not Depressed vs Depressed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can tell that there is a significant imbalance between depressed and not depressed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the Id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropId(data):\n",
    "    data = data.drop(columns=[\"id\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the name column, Replacing each name with its frequency in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_freq(data):\n",
    "    data[\"Name\"] = data[\"Name\"].map(data[\"Name\"].value_counts())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding gender column 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_encode(data):\n",
    "    data[\"Gender\"] = data[\"Gender\"].map({\"Female\": 0, \"Male\": 1 })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age, changing the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_integer(data):\n",
    "    data[\"Age\"] = data[\"Age\"].astype(int)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Academic Preessure, inserting 0 in the empty slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def academic_pressure(data):\n",
    "    data[\"Academic Pressure\"] = np.where(\n",
    "        data[\"Academic Pressure\"].isnull(), 0,\n",
    "        data[\"Academic Pressure\"]\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Family History of Mental Illnes. -> 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def family_mental_illness(data):\n",
    "    data[\"Family History of Mental Illness\"] = np.where(\n",
    "        data[\"Family History of Mental Illness\"] == \"No\", 0,\n",
    "        np.where(\n",
    "            data[\"Family History of Mental Illness\"] == \"Yes\", 1,\n",
    "            data[\"Family History of Mental Illness\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert to integer type\n",
    "    data[\"Family History of Mental Illness\"] = data[\"Family History of Mental Illness\"].astype(int)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you ever had suicidal thoughts? -> 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suicidal_thoughts(data):\n",
    "    data[\"Have you ever had suicidal thoughts ?\"] = np.where(\n",
    "        data[\"Have you ever had suicidal thoughts ?\"] == \"No\", 0,\n",
    "        np.where(\n",
    "            data[\"Have you ever had suicidal thoughts ?\"] == \"Yes\", 1,\n",
    "            data[\"Have you ever had suicidal thoughts ?\"]\n",
    "        )\n",
    "    )\n",
    "    data[\"Have you ever had suicidal thoughts ?\"] = data[\"Have you ever had suicidal thoughts ?\"].astype(int)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Financial stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def financial_stress(data):\n",
    "    data[\"Financial Stress\"] = np.where(\n",
    "        data[\"Financial Stress\"].isnull(), round(data[\"Financial Stress\"].mean()), data[\"Financial Stress\"]\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working Professional or Student. -> 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def working_or_student(data):\n",
    "    data[\"Working Professional or Student\"] = np.where(\n",
    "        data[\"Working Professional or Student\"] == \"Working Professional\", 0,\n",
    "        np.where(\n",
    "            data[\"Working Professional or Student\"] == \"Student\", 1,\n",
    "            data[\"Working Professional or Student\"]\n",
    "        )\n",
    "    )       \n",
    "\n",
    "    # Convert to integer type\n",
    "    data[\"Working Professional or Student\"] = data[\"Working Professional or Student\"].astype(int)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profession. Inserting Student in the empty slots if the sample is a student and missing profession if it is empty and the sample is not a student. also inserting \"Missing Profession\" in the professions that has less than 10 occurrences in the dataset because the majority of these are wrong information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profession(data):\n",
    "    # Count occurrences of each profession\n",
    "    profession_counts = data[\"Profession\"].value_counts()\n",
    "    \n",
    "    # Update the \"Profession\" column\n",
    "    data[\"Profession\"] = np.where(\n",
    "        data[\"Profession\"].isnull() & (data[\"Working Professional or Student\"] == 1),\n",
    "        \"Student\",\n",
    "        np.where(\n",
    "            data[\"Profession\"].isnull() & (data[\"Working Professional or Student\"] == 0), \n",
    "            \"Missing Profession\", \n",
    "            np.where(\n",
    "                data[\"Profession\"].map(profession_counts) < 6,  # If occurrence < 10, set to \"Missing Profession\"\n",
    "                \"Missing Profession\",\n",
    "                data[\"Profession\"]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorizing Professions and giving each category a number. the number is a rating from 0 to 10 on how happy people are in given categories. We used chatGPT to rate them based on different logical points. see professions_mapping dictionary for description of the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_profession(profession):\n",
    "    if pd.isna(profession) or \"missing\" in profession.lower():\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    profession = profession.lower().strip()\n",
    "\n",
    "    # Technology\n",
    "    tech_keywords = [\"software\", \"data scientist\", \"ux/ui\", \"developer\", \"engineer\"]\n",
    "    if any(keyword in profession for keyword in tech_keywords):\n",
    "        return \"Technology\"\n",
    "\n",
    "    # Finance\n",
    "    finance_keywords = [\"accountant\", \"financial\", \"investment\", \"banker\", \"analyst\"]\n",
    "    if any(keyword in profession for keyword in finance_keywords):\n",
    "        return \"Finance\"\n",
    "\n",
    "    # Healthcare\n",
    "    healthcare_keywords = [\"doctor\", \"pharmacist\", \"dentist\", \"nurse\"]\n",
    "    if any(keyword in profession for keyword in healthcare_keywords):\n",
    "        return \"Healthcare\"\n",
    "\n",
    "    # Education\n",
    "    education_keywords = [\"teacher\", \"professor\", \"educational\"]\n",
    "    if any(keyword in profession for keyword in education_keywords):\n",
    "        return \"Education\"\n",
    "\n",
    "    # Engineering\n",
    "    engineering_keywords = [\"civil engineer\", \"mechanical engineer\", \"architect\"]\n",
    "    if any(keyword in profession for keyword in engineering_keywords):\n",
    "        return \"Engineering\"\n",
    "\n",
    "    # Marketing & Sales\n",
    "    marketing_keywords = [\"marketing\", \"sales\", \"digital marketer\", \"content writer\"]\n",
    "    if any(keyword in profession for keyword in marketing_keywords):\n",
    "        return \"Marketing/Sales\"\n",
    "\n",
    "    # Trade & Manual Work\n",
    "    trade_keywords = [\"electrician\", \"plumber\", \"chef\", \"mechanic\"]\n",
    "    if any(keyword in profession for keyword in trade_keywords):\n",
    "        return \"Trade\"\n",
    "\n",
    "    # Legal\n",
    "    legal_keywords = [\"lawyer\", \"judge\", \"legal\"]\n",
    "    if any(keyword in profession for keyword in legal_keywords):\n",
    "        return \"Legal\"\n",
    "\n",
    "    # Consulting\n",
    "    consulting_keywords = [\"consultant\", \"business analyst\"]\n",
    "    if any(keyword in profession for keyword in consulting_keywords):\n",
    "        return \"Consulting\"\n",
    "\n",
    "    # Other / Unknown\n",
    "    return \"Other\"\n",
    "\n",
    "def Prefession_categorization(data):\n",
    "    # Apply categorization\n",
    "    data[\"Profession_Category\"] = data[\"Profession\"].apply(categorize_profession)\n",
    "\n",
    "    # Encoding Options\n",
    "\n",
    "    ## Option 1: Ordinal Encoding (useful if there's a natural order)\n",
    "    profession_mapping = {\n",
    "    \"Unknown\": 0,           # No data on job satisfaction\n",
    "    \"Other\": 4,             # Mixed bag, depends on the job\n",
    "    \"Trade\": 6,             # Skilled trades often provide satisfaction, but physically demanding\n",
    "    \"Marketing/Sales\": 5,   # High variability, stressful but rewarding\n",
    "    \"Consulting\": 5,        # High pay but often high stress & long hours\n",
    "    \"Education\": 7,         # Fulfilling, but pay can be low and stress can be high\n",
    "    \"Finance\": 6,           # High pay, but long hours and stress\n",
    "    \"Engineering\": 7,       # Good pay, problem-solving, but deadlines can be stressful\n",
    "    \"Healthcare\": 6,        # Rewarding but high stress and burnout risk\n",
    "    \"Legal\": 5,             # High salary, but stressful and demanding\n",
    "    \"Technology\": 8         # High salary, flexibility, remote work options\n",
    "}\n",
    "\n",
    "\n",
    "    data[\"Profession_Encoded\"] = data[\"Profession_Category\"].map(profession_mapping)\n",
    "\n",
    "    data[\"Profession\"] = data[\"Profession_Encoded\"]\n",
    "\n",
    "    data = data.drop(columns=[\"Profession_Encoded\", \"Profession_Category\"])\n",
    "    return data\n",
    "    ## Option 2: One-Hot Encoding (better for categorical data)\n",
    "    # data = pd.get_dummies(data, columns=[\"Profession_Category\"], prefix=\"Profession\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def work_pressure(data):\n",
    "    data[\"Work Pressure\"] = np.where(\n",
    "        data[\"Work Pressure\"].isnull(), 0.0, data[\"Work Pressure\"]\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CGPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cgpa(data):\n",
    "    data[\"CGPA\"] = np.where(\n",
    "        data[\"CGPA\"].isnull(), round(data[\"CGPA\"].mean(), 2), data[\"CGPA\"]\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study- and Job Satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def satisfaction(data):\n",
    "    satisfaction = np.where(\n",
    "        data[\"Study Satisfaction\"].notnull(), data[\"Study Satisfaction\"], np.where(\n",
    "            data[\"Job Satisfaction\"].notnull(), data[\"Job Satisfaction\"], 0\n",
    "        )\n",
    "    )\n",
    "    data[\"Job Satisfaction\"] = satisfaction\n",
    "    data = data.drop(columns=\"Study Satisfaction\")\n",
    "    data.rename(columns={\"Job Satisfaction\": \"Satisfaction\"}, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sleep Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_duration(data):\n",
    "    data[\"Sleep Duration\"] = np.where(\n",
    "        data[\"Sleep Duration\"] == \"Less than 5 hours\", 1, np.where(\n",
    "            data[\"Sleep Duration\"] == \"5-6 hours\", 2, np.where(\n",
    "                data[\"Sleep Duration\"] == \"7-8 hours\", 3, np.where(\n",
    "                    data[\"Sleep Duration\"] == \"More than 8 hours\", 4, 1\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dietary Habits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diet(data):\n",
    "    data[\"Dietary Habits\"] = np.where(\n",
    "        data[\"Dietary Habits\"] == \"Healthy\", 2, np.where(\n",
    "            data[\"Dietary Habits\"] == \"Moderate\", 1, np.where(\n",
    "                data[\"Dietary Habits\"] == \"Unhealthy\", 0, 1\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_degree(degree):\n",
    "\n",
    "    \"\"\"\n",
    "    Categorizes the differnet degrees into Schoo, Bachelors, Masters, Doctrate, professional and other\n",
    "    \"\"\"\n",
    "    \n",
    "    degree = str(degree).strip().lower()\n",
    "\n",
    "    # School-Level\n",
    "    if \"class 11\" in degree or \"class 12\" in degree:\n",
    "        return \"School\"\n",
    "\n",
    "    # Bachelor's Degrees\n",
    "    bachelor_keywords = [\"b.\", \"b \", \"bachelor\", \"bcom\", \"bsc\", \"btech\", \"be\", \"bba\", \"bca\", \"ba\", \"b.ed\", \"b.arch\"]\n",
    "    if any(keyword in degree for keyword in bachelor_keywords):\n",
    "        return \"Bachelors\"\n",
    "\n",
    "    # Master's Degrees\n",
    "    master_keywords = [\"m.\", \"m \", \"master\", \"mba\", \"mcom\", \"msc\", \"mtech\", \"me\", \"mca\", \"m.ed\", \"mpharm\", \"m.arch\"]\n",
    "    if any(keyword in degree for keyword in master_keywords):\n",
    "        return \"Masters\"\n",
    "\n",
    "    # Doctoral Degrees\n",
    "    if \"phd\" in degree:\n",
    "        return \"Doctorate\"\n",
    "\n",
    "    # Professional Degrees\n",
    "    professional_keywords = [\"mbbs\", \"md\", \"llb\", \"llm\"]\n",
    "    if any(keyword in degree for keyword in professional_keywords):\n",
    "        return \"Professional\"\n",
    "\n",
    "    # Unknown / Noisy Data\n",
    "    return \"Other\"\n",
    "\n",
    "def degree(data):\n",
    "    \n",
    "    # Apply categorization\n",
    "    data[\"Degree\"] = data[\"Degree\"].apply(categorize_degree)\n",
    "\n",
    "\n",
    "    # Define an ordinal mapping\n",
    "    degree_mapping = {\n",
    "        \"School\": 1,\n",
    "        \"Bachelors\": 2,\n",
    "        \"Masters\": 3,\n",
    "        \"Professional\": 4,\n",
    "        \"Doctorate\": 5,\n",
    "        \"Other\": 0  # Keep 'Other' at the highest level or remove it depending on the approach\n",
    "    }\n",
    "\n",
    "    # Apply mapping\n",
    "    data[\"Degree\"] = data[\"Degree\"].map(degree_mapping)\n",
    "\n",
    "    return data \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load city data\n",
    "city_data_path = \"./Data/indian_cities_data.csv\"\n",
    "df_cities = pd.read_csv(city_data_path)\n",
    "\n",
    "def city_one_hot(data):\n",
    "    # Find the index of the \"City\" column\n",
    "    city_index = data.columns.get_loc(\"City\")\n",
    "\n",
    "    # Merge the data with city information, excluding \"Main Language\"\n",
    "    merged_data = data.merge(df_cities.drop(columns=[\"Main Language\"]), on=\"City\", how=\"left\")\n",
    "\n",
    "    # Calculate the mean values for missing cities\n",
    "    mean_values = merged_data[[\"Population\", \"Density (per km)\", \"Literacy Rate (%)\", \"Sex Ratio\"]].mean()\n",
    "\n",
    "    # Replace NaN values with the mean of the respective column\n",
    "    merged_data.fillna(mean_values, inplace=True)\n",
    "\n",
    "    # Drop the original \"City\" column\n",
    "    merged_data.drop(columns=[\"City\"], inplace=True)\n",
    "\n",
    "    # Reorder columns to place new city-related columns where \"City\" was\n",
    "    city_columns = [\"Population\", \"Density (per km)\", \"Literacy Rate (%)\", \"Sex Ratio\"]\n",
    "    cols = merged_data.columns.tolist()\n",
    "\n",
    "    # Move the new city-related columns to the correct index\n",
    "    for col in reversed(city_columns):\n",
    "        cols.insert(city_index, cols.pop(cols.index(col)))\n",
    "\n",
    "    # Reorder the dataframe\n",
    "    merged_data = merged_data[cols]\n",
    "\n",
    "    return merged_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(data):\n",
    "\n",
    "    data = dropId(data)\n",
    "    data = name_freq(data)\n",
    "    data = gender_encode(data)\n",
    "    data = age_integer(data)\n",
    "    data = academic_pressure(data)\n",
    "    data = family_mental_illness(data)\n",
    "    data = suicidal_thoughts(data)\n",
    "    data = working_or_student(data)\n",
    "    data = profession(data)\n",
    "    data = work_pressure(data)\n",
    "    data = cgpa(data)\n",
    "    data = satisfaction(data)\n",
    "    data = sleep_duration(data)\n",
    "    data = diet(data)\n",
    "    data = degree(data)\n",
    "    data = Prefession_categorization(data)\n",
    "    data = city_one_hot(data)\n",
    "    data = financial_stress(data)\n",
    "    return data\n",
    "\n",
    "training_data = pre_processing(data)\n",
    "test_data = pre_processing(test)\n",
    "test_data.to_csv(\"test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is The function for Normalizing (0-1) the dataset. This is necessary or helpful if we want to use:\n",
    "\n",
    "| Learning Algorithm |\n",
    "|--------------------|\n",
    "| Logistic Regression |\n",
    "| SVM |\n",
    "| Neural Network - better |\n",
    "| K-Means |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(training_data, test_data, columns_to_scale):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(training_data[columns_to_scale])\n",
    "    training_data[columns_to_scale] = scaler.transform(training_data[columns_to_scale])\n",
    "    test_data[columns_to_scale] = scaler.transform(test_data[columns_to_scale])\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is The function for Standardising the dataset. This is necessary or helpful if we want to use:\n",
    "\n",
    "| Learning Algorithm |\n",
    "|--------------------|\n",
    "| Logistic Regression |\n",
    "| SVM - better |\n",
    "| Neural Network |\n",
    "| PCA |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_dataset(training_data, test_data, columns_to_scale):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(training_data[columns_to_scale])\n",
    "    training_data[columns_to_scale] = scaler.transform(training_data[columns_to_scale])\n",
    "    test_data[columns_to_scale] = scaler.transform(test_data[columns_to_scale])\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns that needs to be scaled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_scale = [\"Population\", \"Density (per km)\", \"Literacy Rate (%)\", \"CGPA\", \"Work/Study Hours\"]\n",
    "\n",
    "def scale(method: str, training_data, test_data):\n",
    "    if method == \"Standardize\":\n",
    "        training_data, test_data = standardize_dataset(training_data, test_data, columns_to_scale)\n",
    "    else:\n",
    "        training_data, test_data = normalize_dataset(training_data, test_data, columns_to_scale)\n",
    "    return training_data, test_data\n",
    "\n",
    "# Denne funksjonen blir ikke brukt i koden\n",
    "# TODO: Sjekk om den er ndvendig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into Train and val, also extracting the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_data(seed=0, train_size=0.8, data=training_data):\n",
    "    np.random.seed(seed)\n",
    "    X = data.drop(columns=[\"Depression\"])\n",
    "    y = data[\"Depression\"]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=train_size, random_state=seed, stratify=y)\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out Random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test method to be used anytime you want to try your model. Prints the submission in \"submission.csv\" in the working directory if you want to submit, but also tests it on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data, X_val, y_val, test_or_val=\"val\"):\n",
    "    if test_or_val == \"test\":\n",
    "        predictions = model.predict(test_data)\n",
    "\n",
    "        submission = pd.DataFrame({\n",
    "            \"id\": test[\"id\"], \n",
    "            \"Depression\": predictions\n",
    "        })\n",
    "        submission.to_csv(\"submission.csv\", index=False)\n",
    "        print(\"test successfully executed, results in submission.csv\")\n",
    "    elif test_or_val == \"val\":\n",
    "        predictions = model.predict(X_val)\n",
    "        accuracy = accuracy_score(predictions, y_val)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the training data into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = split_training_data(seed=seed, data=training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best Hyperparameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "def Random_forest(seed, X_train, y_train):\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [10, 20, 30],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [200],\n",
    "        \"max_depth\": [20],\n",
    "        \"min_samples_split\": [5],\n",
    "        \"min_samples_leaf\": [1]\n",
    "    }\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    rf = RandomForestClassifier(random_state=seed)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=rf,\n",
    "        param_grid=param_grid,\n",
    "        cv=kf,\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1,  # Use all available CPU cores\n",
    "        verbose=1   # Shows progress\n",
    "    )\n",
    "\n",
    "    # No scaling required\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    \n",
    "    model = RandomForestClassifier(max_depth=best_params[\"max_depth\"], \n",
    "                            min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "                            min_samples_split=best_params[\"min_samples_split\"],\n",
    "                            n_estimators=best_params[\"n_estimators\"],\n",
    "                            class_weight=\"balanced\"\n",
    "                            )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "base_model = Random_forest(seed, X_train, y_train)\n",
    "# print(\"Random forest validation accuracy score:\")\n",
    "# RF_accuracy_score = test_model(model, test_data, X_val, y_val, \"val\")\n",
    "# print(RF_accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Random Forest classifier - Magnus Paper implementation\n",
    "\n",
    "[Improved Random forest paper](https://www.sciencedirect.com/science/article/pii/S0957417423020511) <--- link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Baselearners.ImprovedRandomForest import ImprovedRandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypertune parameters with grid search method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def hypertuned_model(improved_training_data):\n",
    "\n",
    "    parameters = {\n",
    "        \"N_final\": [50, 100, 150, 200],\n",
    "        \"m\": [0.1, 0.15, 0.3, 0.5, 0.7],\n",
    "        \"max_depth\": [8, 10, 12, 14, 15, 17, 20],\n",
    "        \"min_samples_leaf\": [2, 4],\n",
    "        \"min_samples_split\": [2, 3, 4]\n",
    "    }\n",
    "\n",
    "    # The best parameters from tuning\n",
    "    parameters = {\n",
    "        \"N_final\": [200],\n",
    "        \"m\": [0.7],\n",
    "        \"max_depth\": [15],\n",
    "        \"min_samples_leaf\": [4],\n",
    "        \"min_samples_split\": [2]\n",
    "    }\n",
    "\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    n_estimators = parameters[\"N_final\"]\n",
    "\n",
    "    # Gives an overview of the computational cost of the hypertuning - for time estamation\n",
    "    total_candidates = 1\n",
    "    for p in parameters:\n",
    "        total_candidates = total_candidates * len(parameters[p])\n",
    "    print(\"The total amount of candidates: \", total_candidates)\n",
    "\n",
    "    best_parameters = {\n",
    "        \"N_final\": None,\n",
    "        \"m\": None,\n",
    "        \"max_depth\": None,\n",
    "        \"min_samples_leaf\": None,\n",
    "        \"min_samples_split\": None\n",
    "    }\n",
    "    best_accuracy_score = 0.0\n",
    "    best_model = None\n",
    "    candidates = total_candidates\n",
    "    for N_final in parameters[\"N_final\"]:\n",
    "        for m in parameters[\"m\"]:\n",
    "            for max_depth in parameters[\"max_depth\"]:\n",
    "                for min_samples_leaf in parameters[\"min_samples_leaf\"]:\n",
    "                    for min_samples_split in parameters[\"min_samples_split\"]:\n",
    "                        # print(f\"Testing the parameters: {N_final}, {m}, {max_depth}, {min_samples_leaf}, {min_samples_split}.\")\n",
    "                        model = ImprovedRandomForest(N_final, m, max_depth, min_samples_leaf, min_samples_split, improved_training_data)\n",
    "\n",
    "                        # validationset score\n",
    "                        val_accuracy = test_model(model, test_data, X_val, y_val, \"val\")\n",
    "                        val_accuracies.append(val_accuracy)\n",
    "\n",
    "                        # trainingset score\n",
    "                        train_accuracy = test_model(model, test_data, X_train, y_train, \"val\")\n",
    "                        train_accuracies.append(train_accuracy)\n",
    "\n",
    "                        if val_accuracy > best_accuracy_score:\n",
    "                            print(f\"Best parameters so far: {N_final}, {m}, {max_depth}, {min_samples_leaf}, {min_samples_split}, with accuracy: {round(val_accuracy, 4)}\")\n",
    "                            best_parameters = {\n",
    "                                \"N_final\": N_final,\n",
    "                                \"m\": m,\n",
    "                                \"max_depth\": max_depth,\n",
    "                                \"min_samples_leaf\": min_samples_leaf,\n",
    "                                \"min_samples_split\": min_samples_split\n",
    "                            }\n",
    "                            best_model = model\n",
    "                            best_accuracy_score = val_accuracy\n",
    "                        candidates -= 1\n",
    "                        print(f\"Remaining candidates: {candidates}/{total_candidates}\")\n",
    "    print(f\"The best performing model with an accuracy score: {round(best_accuracy_score, 4)}, had these parameters:\")\n",
    "    print(f\"N_final: {best_parameters['N_final']}\\nm: {best_parameters['m']}\\nmax_depth: {best_parameters['max_depth']}\\nmin_samples_leaf: {best_parameters['min_samples_leaf']}\\nmin_samples_split: {best_parameters['min_samples_split']}\")\n",
    "    return best_model, best_parameters, round(best_accuracy_score, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base learners for selection net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Name  Gender  Age  Population  Density (per km)  Literacy Rate (%)  \\\n",
      "0  2045       0   49   1618879.0             5200.0              85.77   \n",
      "1   963       1   26   1198491.0             7300.0              79.27   \n",
      "2   730       1   33   1728128.0             2500.0              81.79   \n",
      "3   730       1   22  12442373.0            20482.0              89.73   \n",
      "4   499       0   30   2765348.0             6900.0              82.42   \n",
      "\n",
      "   Sex Ratio  Working Professional or Student  Profession  Academic Pressure  \\\n",
      "0      850.0                                0           6                0.0   \n",
      "1      887.0                                0           7                0.0   \n",
      "2      978.0                                1           4                5.0   \n",
      "3      853.0                                0           7                0.0   \n",
      "4      857.0                                0           6                0.0   \n",
      "\n",
      "   ...  CGPA  Satisfaction  Sleep Duration  Dietary Habits  Degree  \\\n",
      "0  ...  7.66           2.0               4               2       0   \n",
      "1  ...  7.66           3.0               1               0       4   \n",
      "2  ...  8.97           2.0               2               2       2   \n",
      "3  ...  7.66           1.0               1               1       2   \n",
      "4  ...  7.66           1.0               2               0       2   \n",
      "\n",
      "   Have you ever had suicidal thoughts ?  Work/Study Hours  Financial Stress  \\\n",
      "0                                      0               1.0               2.0   \n",
      "1                                      1               7.0               3.0   \n",
      "2                                      1               3.0               1.0   \n",
      "3                                      1              10.0               1.0   \n",
      "4                                      1               9.0               4.0   \n",
      "\n",
      "   Family History of Mental Illness  Depression  \n",
      "0                                 0           0  \n",
      "1                                 0           1  \n",
      "2                                 0           1  \n",
      "3                                 1           1  \n",
      "4                                 1           0  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "   Name  Gender  Age  Population  Density (per km)  Literacy Rate (%)  \\\n",
      "0   717       1   53   1728128.0             2500.0              81.79   \n",
      "1   897       0   58   4496694.0            22000.0              86.31   \n",
      "2   296       1   53   3046163.0             6500.0              72.73   \n",
      "3   228       0   23   1286678.0             8500.0              87.80   \n",
      "4   609       1   47   1246381.0             9088.0              91.37   \n",
      "\n",
      "   Sex Ratio  Working Professional or Student  Profession  Academic Pressure  \\\n",
      "0      978.0                                0           5                0.0   \n",
      "1      908.0                                0           7                0.0   \n",
      "2      900.0                                0           7                0.0   \n",
      "3      908.0                                1           4                5.0   \n",
      "4      920.0                                0           7                0.0   \n",
      "\n",
      "   Work Pressure  CGPA  Satisfaction  Sleep Duration  Dietary Habits  Degree  \\\n",
      "0            2.0  7.67           5.0               1               1       4   \n",
      "1            2.0  7.67           4.0               1               1       2   \n",
      "2            4.0  7.67           1.0               3               1       2   \n",
      "3            0.0  6.84           1.0               4               1       2   \n",
      "4            5.0  7.67           5.0               3               1       2   \n",
      "\n",
      "   Have you ever had suicidal thoughts ?  Work/Study Hours  Financial Stress  \\\n",
      "0                                      0               9.0               3.0   \n",
      "1                                      0               6.0               4.0   \n",
      "2                                      1              12.0               4.0   \n",
      "3                                      1              10.0               4.0   \n",
      "4                                      1               3.0               4.0   \n",
      "\n",
      "   Family History of Mental Illness  \n",
      "0                                 1  \n",
      "1                                 0  \n",
      "2                                 0  \n",
      "3                                 0  \n",
      "4                                 0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize_dataset(train_data, test_data):\n",
    "    # Identify binary columns (values only 0 or 1)\n",
    "    binary_columns = [col for col in train_data.columns if train_data[col].dropna().isin([0, 1]).all()]\n",
    "    \n",
    "    # Identify continuous numerical columns (everything else)\n",
    "    continuous_columns = [col for col in train_data.columns if col not in binary_columns]\n",
    "\n",
    "    # Initialize MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit on training data only, then transform both training and test data\n",
    "    train_data_scaled = train_data.copy()\n",
    "    test_data_scaled = test_data.copy()\n",
    "\n",
    "    train_data_scaled[continuous_columns] = scaler.fit_transform(train_data[continuous_columns])\n",
    "    test_data_scaled[continuous_columns] = scaler.transform(test_data[continuous_columns])  # Only transform\n",
    "\n",
    "    return train_data_scaled, test_data_scaled\n",
    "\n",
    "\n",
    "# Normalize Training and Test Data\n",
    "sn_training_data, sn_test_data = normalize_dataset(training_data, test_data)\n",
    "\n",
    "# Check results\n",
    "print(training_data.head())\n",
    "print(test_data.head())\n",
    "\n",
    "\n",
    "sn_X_train, sn_X_val, sn_y_train, sn_y_val = split_training_data(data=sn_training_data,seed=seed, train_size=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depressed specialized set class distribution:\n",
      "Depression\n",
      "1    0.950023\n",
      "0    0.049977\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Non-depressed specialized set class distribution:\n",
      "Depression\n",
      "0    0.950007\n",
      "1    0.049993\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Assume X_train is your feature DataFrame and y_train is your label Series\n",
    "# First, combine them into one DataFrame.\n",
    "train_df = sn_X_train.copy()\n",
    "train_df['Depression'] = sn_y_train\n",
    "\n",
    "# For the depressed specialized dataset (aim for 95% depression = 1)\n",
    "# 1. Take all depressed samples.\n",
    "df_depr = train_df[train_df['Depression'] == 1]\n",
    "# 2. For non-depressed samples, we need only enough to be 5% of the final set.\n",
    "#    Let D be the number of depressed samples. Then the desired number of non-depressed\n",
    "#    samples is (5/95)*D.\n",
    "D = len(df_depr)\n",
    "non_depr_count = int((5 / 95) * D)\n",
    "df_non_depr = train_df[train_df['Depression'] == 0]\n",
    "# Randomly sample the required non-depressed samples.\n",
    "df_non_depr_sample = df_non_depr.sample(n=non_depr_count, random_state=seed)\n",
    "\n",
    "# Combine to form the depressed-enriched specialized dataset.\n",
    "df_special_depr = pd.concat([df_depr, df_non_depr_sample])\n",
    "\n",
    "# For the non-depressed specialized dataset (aim for 95% depression = 0)\n",
    "# 1. Take all non-depressed samples.\n",
    "df_non_depr = train_df[train_df['Depression'] == 0]\n",
    "# 2. Compute the number of depressed samples needed to be 5% of this dataset.\n",
    "ND = len(df_non_depr)\n",
    "depr_count = int((5 / 95) * ND)\n",
    "df_depr_sample = train_df[train_df['Depression'] == 1].sample(n=depr_count, random_state=seed)\n",
    "\n",
    "# Combine to form the non-depressed-enriched specialized dataset.\n",
    "df_special_non_depr = pd.concat([df_non_depr, df_depr_sample])\n",
    "\n",
    "# Now, separate features and labels for each specialized dataset.\n",
    "X_special_depr = df_special_depr.drop(columns=['Depression'])\n",
    "y_special_depr = df_special_depr['Depression']\n",
    "\n",
    "X_special_non_depr = df_special_non_depr.drop(columns=['Depression'])\n",
    "y_special_non_depr = df_special_non_depr['Depression']\n",
    "\n",
    "# Optionally, print out the class distributions to verify the ratios:\n",
    "print(\"Depressed specialized set class distribution:\")\n",
    "print(y_special_depr.value_counts(normalize=True))\n",
    "# print(y_special_depr.value_counts())\n",
    "print(\"\\nNon-depressed specialized set class distribution:\")\n",
    "print(y_special_non_depr.value_counts(normalize=True))\n",
    "# print(y_special_non_depr.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained logistic_depr with best parameters: {'C': 1}\n",
      "Trained tree_depr with best parameters: {'max_depth': 5, 'min_samples_split': 2}\n",
      "Trained rf_depr with best parameters: {'max_depth': 20, 'n_estimators': 100}\n",
      "Trained xgb_depr with best parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "Trained ada_depr with best parameters: {'learning_rate': 1.0, 'n_estimators': 200}\n",
      "\n",
      "\n",
      "Trained logistic_nondepr with best parameters: {'C': 1}\n",
      "Trained tree_nondepr with best parameters: {'max_depth': 5, 'min_samples_split': 2}\n",
      "Trained rf_nondepr with best parameters: {'max_depth': None, 'n_estimators': 100}\n",
      "Trained xgb_nondepr with best parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "Trained ada_nondepr with best parameters: {'learning_rate': 0.5, 'n_estimators': 200}\n",
      "\n",
      "Total specialized base learners trained: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.ensemble._weight_boosting\")\n",
    "\n",
    "# Define parameter grids for each model type.\n",
    "logistic_params = {'C': [0.01, 0.1, 1, 10]}\n",
    "tree_params = {'max_depth': [None, 5, 10], 'min_samples_split': [2, 5, 10]}\n",
    "rf_params = {'n_estimators': [100, 200], 'max_depth': [None, 10, 20]}\n",
    "xgb_params = {'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7], 'n_estimators': [100, 200]}\n",
    "ada_params = {'n_estimators': [50, 100, 200], 'learning_rate': [0.5, 1.0, 1.5]}\n",
    "\n",
    "# Create dictionaries to hold the specialized learners.\n",
    "# We want 5 models for depression = 1 (depressed) and 5 for depression = 0 (non-depressed).\n",
    "\n",
    "specialized_learners = {}\n",
    "\n",
    "# --- For the Depressed Group (label == 1) ---\n",
    "# Assume X_special_depr and y_special_depr are the enriched training data for depression=1.\n",
    "# Logistic Regression specialized for depressed samples.\n",
    "logistic_depr_grid = GridSearchCV(LogisticRegression(max_iter=1000), logistic_params, cv=5, scoring='accuracy')\n",
    "logistic_depr_grid.fit(X_special_depr, y_special_depr)\n",
    "specialized_learners[\"logistic_depr\"] = logistic_depr_grid.best_estimator_\n",
    "print(\"Trained logistic_depr with best parameters:\", logistic_depr_grid.best_params_)\n",
    "\n",
    "# Decision Tree specialized for depressed samples.\n",
    "tree_depr_grid = GridSearchCV(DecisionTreeClassifier(random_state=seed), tree_params, cv=5, scoring='accuracy')\n",
    "tree_depr_grid.fit(X_special_depr, y_special_depr)\n",
    "specialized_learners[\"tree_depr\"] = tree_depr_grid.best_estimator_\n",
    "print(\"Trained tree_depr with best parameters:\", tree_depr_grid.best_params_)\n",
    "\n",
    "# Random Forest specialized for depressed samples.\n",
    "rf_depr_grid = GridSearchCV(RandomForestClassifier(random_state=seed), rf_params, cv=5, scoring='accuracy')\n",
    "rf_depr_grid.fit(X_special_depr, y_special_depr)\n",
    "specialized_learners[\"rf_depr\"] = rf_depr_grid.best_estimator_\n",
    "print(\"Trained rf_depr with best parameters:\", rf_depr_grid.best_params_)\n",
    "\n",
    "# XGBoost specialized for depressed samples.\n",
    "xgb_depr_grid = GridSearchCV(xgb.XGBClassifier(eval_metric='logloss'), xgb_params, cv=5, scoring='accuracy')\n",
    "xgb_depr_grid.fit(X_special_depr, y_special_depr)\n",
    "specialized_learners[\"xgb_depr\"] = xgb_depr_grid.best_estimator_\n",
    "print(\"Trained xgb_depr with best parameters:\", xgb_depr_grid.best_params_)\n",
    "\n",
    "# AdaBoost specialized for depressed samples.\n",
    "ada_depr_grid = GridSearchCV(AdaBoostClassifier(algorithm='SAMME', random_state=seed), ada_params, cv=5, scoring='accuracy')\n",
    "ada_depr_grid.fit(X_special_depr, y_special_depr)\n",
    "specialized_learners[\"ada_depr\"] = ada_depr_grid.best_estimator_\n",
    "print(\"Trained ada_depr with best parameters:\", ada_depr_grid.best_params_)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- For the Non-Depressed Group (label == 0) ---\n",
    "# Assume X_special_non_depr and y_special_non_depr are the enriched training data for depression=0.\n",
    "# Logistic Regression specialized for non-depressed samples.\n",
    "logistic_non_depr_grid = GridSearchCV(LogisticRegression(max_iter=1000), logistic_params, cv=5, scoring='accuracy')\n",
    "logistic_non_depr_grid.fit(X_special_non_depr, y_special_non_depr)\n",
    "specialized_learners[\"logistic_nondepr\"] = logistic_non_depr_grid.best_estimator_\n",
    "print(\"Trained logistic_nondepr with best parameters:\", logistic_non_depr_grid.best_params_)\n",
    "\n",
    "# Decision Tree specialized for non-depressed samples.\n",
    "tree_non_depr_grid = GridSearchCV(DecisionTreeClassifier(random_state=seed), tree_params, cv=5, scoring='accuracy')\n",
    "tree_non_depr_grid.fit(X_special_non_depr, y_special_non_depr)\n",
    "specialized_learners[\"tree_nondepr\"] = tree_non_depr_grid.best_estimator_\n",
    "print(\"Trained tree_nondepr with best parameters:\", tree_non_depr_grid.best_params_)\n",
    "\n",
    "# Random Forest specialized for non-depressed samples.\n",
    "rf_non_depr_grid = GridSearchCV(RandomForestClassifier(random_state=seed), rf_params, cv=5, scoring='accuracy')\n",
    "rf_non_depr_grid.fit(X_special_non_depr, y_special_non_depr)\n",
    "specialized_learners[\"rf_nondepr\"] = rf_non_depr_grid.best_estimator_\n",
    "print(\"Trained rf_nondepr with best parameters:\", rf_non_depr_grid.best_params_)\n",
    "\n",
    "# XGBoost specialized for non-depressed samples.\n",
    "xgb_non_depr_grid = GridSearchCV(xgb.XGBClassifier(eval_metric='logloss'), xgb_params, cv=5, scoring='accuracy')\n",
    "xgb_non_depr_grid.fit(X_special_non_depr, y_special_non_depr)\n",
    "specialized_learners[\"xgb_nondepr\"] = xgb_non_depr_grid.best_estimator_\n",
    "print(\"Trained xgb_nondepr with best parameters:\", xgb_non_depr_grid.best_params_)\n",
    "\n",
    "# AdaBoost specialized for non-depressed samples.\n",
    "ada_non_depr_grid = GridSearchCV(AdaBoostClassifier(algorithm='SAMME', random_state=seed), ada_params, cv=5, scoring='accuracy')\n",
    "ada_non_depr_grid.fit(X_special_non_depr, y_special_non_depr)\n",
    "specialized_learners[\"ada_nondepr\"] = ada_non_depr_grid.best_estimator_\n",
    "print(\"Trained ada_nondepr with best parameters:\", ada_non_depr_grid.best_params_)\n",
    "\n",
    "print(\"\\nTotal specialized base learners trained:\", len(specialized_learners))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy dynamic feature selection - Isak Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making tensors and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base learners from magnus paper, isaks paper and our base model -> 9 base learners\n",
    "\n",
    "training_with_depressed_only = training_data[training_data[\"Depression\"] == 1]\n",
    "training_with_happy_only = training_data[training_data[\"Depression\"] == 0]\n",
    "\n",
    "X_train_dep, X_val_dep, y_train_dep, y_val_dep = split_training_data(seed, data = df_special_depr)\n",
    "X_train_happy, X_val_happy, y_train_happy, y_val_happy = split_training_data(seed, data = df_special_non_depr)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "#test\n",
    "\n",
    "#X_test_tensor = torch.tensor(test, dtype=torch.float32)\n",
    "#test_mean = X_test_tensor.mean(dim=0)\n",
    "#X_test_tensor = (X_test_tensor - test_mean)\n",
    "\n",
    "#happy\n",
    "X_train_tensor_happy = torch.tensor(X_train_happy.values, dtype=torch.float32)\n",
    "\n",
    "train_mean_happy = X_train_tensor_happy.mean(dim=0)\n",
    "X_train_tensor_happy = (X_train_tensor_happy - train_mean_happy)\n",
    "\n",
    "\n",
    "y_train_tensor_happy = torch.tensor(y_train_happy.values, dtype=torch.long).view(-1, 1)\n",
    "y_train_tensor_happy = y_train_tensor_happy.squeeze()\n",
    "\n",
    "\n",
    "X_val_tensor_happy = torch.tensor(X_val_happy.values, dtype=torch.float32)\n",
    "val_mean_happy = X_val_tensor_happy.mean(dim=0)\n",
    "X_val_tensor_happy = (X_val_tensor_happy - val_mean_happy)\n",
    "\n",
    "y_val_tensor_happy = torch.tensor(y_val_happy.values, dtype=torch.long).view(-1, 1)\n",
    "y_val_tensor_happy = y_val_tensor_happy.squeeze()\n",
    "\n",
    "#depressed\n",
    "\n",
    "X_train_tensor_dep = torch.tensor(X_train_dep.values, dtype=torch.float32)\n",
    "train_mean_dep = X_train_tensor_dep.mean(dim=0)\n",
    "X_train_tensor_dep = (X_train_tensor_dep - train_mean_dep)\n",
    "\n",
    "y_train_tensor_dep = torch.tensor(y_train_dep.values, dtype=torch.long).view(-1, 1)\n",
    "y_train_tensor_dep = y_train_tensor_dep.squeeze()\n",
    "\n",
    "\n",
    "X_val_tensor_dep = torch.tensor(X_val_dep.values, dtype=torch.float32)\n",
    "val_mean_dep = X_val_tensor_dep.mean(dim=0)\n",
    "X_val_tensor_dep = (X_val_tensor_dep - val_mean_dep)\n",
    "\n",
    "\n",
    "\n",
    "y_val_tensor_dep = torch.tensor(y_val_dep.values, dtype=torch.long).view(-1, 1)\n",
    "y_val_tensor_dep = y_val_tensor_dep.squeeze()\n",
    "\n",
    "#full\n",
    "X_train_tensor_full = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "train_mean_full = X_train_tensor_full.mean(dim=0)\n",
    "X_train_tensor_full = (X_train_tensor_full - train_mean_full)\n",
    "\n",
    "\n",
    "y_train_tensor_full = torch.tensor(y_train.values, dtype=torch.long).view(-1, 1)\n",
    "y_train_tensor_full = y_train_tensor_full.squeeze()\n",
    "\n",
    "\n",
    "X_val_tensor_full = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "val_mean_full = X_val_tensor_full.mean(dim=0)\n",
    "X_val_tensor_full = (X_val_tensor_full - val_mean_full)\n",
    "\n",
    "y_val_tensor_full = torch.tensor(y_val.values, dtype=torch.long).view(-1, 1)\n",
    "y_val_tensor_full = y_val_tensor_full.squeeze()\n",
    "\n",
    "happy_dataset_train = TensorDataset(X_train_tensor_happy, y_train_tensor_happy)\n",
    "happy_dataset_val = TensorDataset(X_val_tensor_happy, y_val_tensor_happy)\n",
    "\n",
    "depressed_dataset_train = TensorDataset(X_train_tensor_dep, y_train_tensor_dep)\n",
    "depressed_dataset_val = TensorDataset(X_val_tensor_dep, y_val_tensor_dep)\n",
    "\n",
    "full_dataset_train = TensorDataset(X_train_tensor_full, y_train_tensor_full)\n",
    "full_dataset_val = TensorDataset(X_val_tensor_full, y_val_tensor_full)\n",
    "\n",
    "\n",
    "\n",
    "happy_train_loader = DataLoader(happy_dataset_train, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "happy_val_loader = DataLoader(happy_dataset_val, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "depressed_train_loader = DataLoader(depressed_dataset_train, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "depressed_val_loader = DataLoader(depressed_dataset_val, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "full_train_loader = DataLoader(full_dataset_train, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "full_val_loader = DataLoader(full_dataset_val, batch_size=batch_size, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the net and pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running Happy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x:\\Anaconda\\envs\\367A\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Epoch 1--------\n",
      "Val loss = 0.1346\n",
      "\n",
      "--------Epoch 2--------\n",
      "Val loss = 0.1307\n",
      "\n",
      "--------Epoch 3--------\n",
      "Val loss = 0.1352\n",
      "\n",
      "--------Epoch 4--------\n",
      "Val loss = 0.1296\n",
      "\n",
      "--------Epoch 5--------\n",
      "Val loss = 0.1328\n",
      "\n",
      "--------Epoch 6--------\n",
      "Val loss = 0.1285\n",
      "\n",
      "--------Epoch 7--------\n",
      "Val loss = 0.1274\n",
      "\n",
      "--------Epoch 8--------\n",
      "Val loss = 0.1302\n",
      "\n",
      "--------Epoch 9--------\n",
      "Val loss = 0.1281\n",
      "\n",
      "--------Epoch 10--------\n",
      "Val loss = 0.1283\n",
      "\n",
      "--------Epoch 11--------\n",
      "Val loss = 0.1268\n",
      "\n",
      "--------Epoch 12--------\n",
      "Val loss = 0.1251\n",
      "\n",
      "--------Epoch 13--------\n",
      "Val loss = 0.1249\n",
      "\n",
      "--------Epoch 14--------\n",
      "Val loss = 0.1296\n",
      "\n",
      "--------Epoch 15--------\n",
      "Val loss = 0.1301\n",
      "\n",
      "--------Epoch 16--------\n",
      "Val loss = 0.1254\n",
      "\n",
      "--------Epoch 17--------\n",
      "Val loss = 0.1271\n",
      "\n",
      "Stopping early at epoch 17\n",
      " Running depressed\n",
      "--------Epoch 1--------\n",
      "Val loss = 0.1478\n",
      "\n",
      "--------Epoch 2--------\n",
      "Val loss = 0.1197\n",
      "\n",
      "--------Epoch 3--------\n",
      "Val loss = 0.1198\n",
      "\n",
      "--------Epoch 4--------\n",
      "Val loss = 0.1256\n",
      "\n",
      "--------Epoch 5--------\n",
      "Val loss = 0.1267\n",
      "\n",
      "--------Epoch 6--------\n",
      "Val loss = 0.1245\n",
      "\n",
      "Stopping early at epoch 6\n",
      " Running full\n",
      "--------Epoch 1--------\n",
      "Val loss = 95.5480\n",
      "\n",
      "--------Epoch 2--------\n",
      "Val loss = 26.0766\n",
      "\n",
      "--------Epoch 3--------\n",
      "Val loss = 0.8783\n",
      "\n",
      "--------Epoch 4--------\n",
      "Val loss = 0.4626\n",
      "\n",
      "--------Epoch 5--------\n",
      "Val loss = 0.4605\n",
      "\n",
      "--------Epoch 6--------\n",
      "Val loss = 0.4616\n",
      "\n",
      "--------Epoch 7--------\n",
      "Val loss = 0.4595\n",
      "\n",
      "--------Epoch 8--------\n",
      "Val loss = 0.4590\n",
      "\n",
      "--------Epoch 9--------\n",
      "Val loss = 0.4582\n",
      "\n",
      "--------Epoch 10--------\n",
      "Val loss = 0.4588\n",
      "\n",
      "--------Epoch 11--------\n",
      "Val loss = 0.4580\n",
      "\n",
      "--------Epoch 12--------\n",
      "Val loss = 0.4593\n",
      "\n",
      "--------Epoch 13--------\n",
      "Val loss = 0.4578\n",
      "\n",
      "--------Epoch 14--------\n",
      "Val loss = 0.4569\n",
      "\n",
      "--------Epoch 15--------\n",
      "Val loss = 0.4566\n",
      "\n",
      "--------Epoch 16--------\n",
      "Val loss = 0.4572\n",
      "\n",
      "--------Epoch 17--------\n",
      "Val loss = 0.4572\n",
      "\n",
      "--------Epoch 18--------\n",
      "Val loss = 0.4547\n",
      "\n",
      "--------Epoch 19--------\n",
      "Val loss = 0.4550\n",
      "\n",
      "--------Epoch 20--------\n",
      "Val loss = 0.4524\n",
      "\n",
      "--------Epoch 21--------\n",
      "Val loss = 0.4548\n",
      "\n",
      "--------Epoch 22--------\n",
      "Val loss = 0.4537\n",
      "\n",
      "--------Epoch 23--------\n",
      "Val loss = 0.4520\n",
      "\n",
      "--------Epoch 24--------\n",
      "Val loss = 0.4523\n",
      "\n",
      "--------Epoch 25--------\n",
      "Val loss = 0.4502\n",
      "\n",
      "--------Epoch 26--------\n",
      "Val loss = 0.4550\n",
      "\n",
      "--------Epoch 27--------\n",
      "Val loss = 0.4515\n",
      "\n",
      "--------Epoch 28--------\n",
      "Val loss = 0.4533\n",
      "\n",
      "--------Epoch 29--------\n",
      "Val loss = 0.4504\n",
      "\n",
      "Stopping early at epoch 29\n"
     ]
    }
   ],
   "source": [
    "input_size = 20\n",
    "output_size = 2 \n",
    "dropout_rate = 0.4\n",
    "hidden_size = 64\n",
    "\n",
    "\n",
    "predicting_net= nn.Sequential(\n",
    "    nn.Linear(2*input_size,hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout_rate),\n",
    "    nn.Linear(hidden_size,hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout_rate),\n",
    "    nn.Linear(hidden_size,output_size))\n",
    "\n",
    "selecting_net = nn.Sequential(\n",
    "    nn.Linear(2 * input_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout_rate),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout_rate),\n",
    "    nn.Linear(hidden_size, input_size ))\n",
    "\n",
    "mask_layer_happy = ds.utils.MaskLayer(append=True)\n",
    "\n",
    "mask_layer_dep = ds.utils.MaskLayer(append=True)\n",
    "\n",
    "mask_layer_full = ds.utils.MaskLayer(append=True)\n",
    "\n",
    "print(' Running Happy')\n",
    "pretrain_happy = MaskingPretrainer(predicting_net, mask_layer_happy)\n",
    "pretrain_happy.fit(\n",
    "    happy_train_loader,\n",
    "    happy_val_loader,\n",
    "    lr=1e-3,\n",
    "    nepochs=100,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    verbose=True)\n",
    "\n",
    "print(' Running depressed')\n",
    "pretrain_dep = MaskingPretrainer(predicting_net, mask_layer_dep)\n",
    "pretrain_dep.fit(\n",
    "    depressed_train_loader,\n",
    "    depressed_val_loader,\n",
    "    lr=1e-3,\n",
    "    nepochs=100,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    verbose=True)\n",
    "\n",
    "print(' Running full')\n",
    "pretrain_full = MaskingPretrainer(predicting_net, mask_layer_full)\n",
    "pretrain_full.fit(\n",
    "    full_train_loader,\n",
    "    full_val_loader,\n",
    "    lr=1e-3,\n",
    "    nepochs=100,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running Happy\n",
      "Starting training with temp = 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x:\\Anaconda\\envs\\367A\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Epoch 1 (1 total)--------\n",
      "Val loss = 0.0904, Zero-temp loss = 0.0920\n",
      "\n",
      "--------Epoch 2 (2 total)--------\n",
      "Val loss = 0.0885, Zero-temp loss = 0.0908\n",
      "\n",
      "--------Epoch 3 (3 total)--------\n",
      "Val loss = 0.0853, Zero-temp loss = 0.0874\n",
      "\n",
      "Stopping temp = 1.0000 at epoch 3\n",
      "\n",
      "Starting training with temp = 0.5623\n",
      "\n",
      "--------Epoch 1 (4 total)--------\n",
      "Val loss = 0.0858, Zero-temp loss = 0.0874\n",
      "\n",
      "--------Epoch 2 (5 total)--------\n",
      "Val loss = 0.0849, Zero-temp loss = 0.0858\n",
      "\n",
      "--------Epoch 3 (6 total)--------\n",
      "Val loss = 0.0849, Zero-temp loss = 0.0862\n",
      "\n",
      "Stopping temp = 0.5623 at epoch 3\n",
      "\n",
      "Starting training with temp = 0.3162\n",
      "\n",
      "--------Epoch 1 (7 total)--------\n",
      "Val loss = 0.0861, Zero-temp loss = 0.0863\n",
      "\n",
      "--------Epoch 2 (8 total)--------\n",
      "Val loss = 0.0862, Zero-temp loss = 0.0864\n",
      "\n",
      "--------Epoch 3 (9 total)--------\n",
      "Val loss = 0.0860, Zero-temp loss = 0.0863\n",
      "\n",
      "Stopping temp = 0.3162 at epoch 3\n",
      "\n",
      "Starting training with temp = 0.1778\n",
      "\n",
      "--------Epoch 1 (10 total)--------\n",
      "Val loss = 0.0848, Zero-temp loss = 0.0849\n",
      "\n",
      "--------Epoch 2 (11 total)--------\n",
      "Val loss = 0.0867, Zero-temp loss = 0.0867\n",
      "\n",
      "--------Epoch 3 (12 total)--------\n",
      "Val loss = 0.0848, Zero-temp loss = 0.0848\n",
      "\n",
      "Stopping temp = 0.1778 at epoch 3\n",
      "\n",
      "Starting training with temp = 0.1000\n",
      "\n",
      "--------Epoch 1 (13 total)--------\n",
      "Val loss = 0.0856, Zero-temp loss = 0.0856\n",
      "\n",
      "--------Epoch 2 (14 total)--------\n",
      "Val loss = 0.0856, Zero-temp loss = 0.0856\n",
      "\n",
      "--------Epoch 3 (15 total)--------\n",
      "Val loss = 0.0851, Zero-temp loss = 0.0851\n",
      "\n",
      "Stopping temp = 0.1000 at epoch 3\n",
      "\n",
      " Running depressed\n",
      "Starting training with temp = 1.0000\n",
      "\n",
      "--------Epoch 1 (1 total)--------\n",
      "Val loss = 0.1281, Zero-temp loss = 0.1318\n",
      "\n",
      "--------Epoch 2 (2 total)--------\n",
      "Val loss = 0.1010, Zero-temp loss = 0.1049\n",
      "\n",
      "--------Epoch 3 (3 total)--------\n",
      "Val loss = 0.0930, Zero-temp loss = 0.0999\n",
      "\n",
      "Stopping temp = 1.0000 at epoch 3\n",
      "\n",
      "Starting training with temp = 0.5623\n",
      "\n",
      "--------Epoch 1 (4 total)--------\n",
      "Val loss = 0.0794, Zero-temp loss = 0.0833\n",
      "\n",
      "--------Epoch 2 (5 total)--------\n",
      "Val loss = 0.0764, Zero-temp loss = 0.0785\n",
      "\n",
      "--------Epoch 3 (6 total)--------\n",
      "Val loss = 0.0783, Zero-temp loss = 0.0797\n",
      "\n",
      "Stopping temp = 0.5623 at epoch 3\n",
      "\n",
      "Starting training with temp = 0.3162\n",
      "\n",
      "--------Epoch 1 (7 total)--------\n",
      "Val loss = 0.0724, Zero-temp loss = 0.0729\n",
      "\n",
      "--------Epoch 2 (8 total)--------\n",
      "Val loss = 0.0712, Zero-temp loss = 0.0716\n",
      "\n",
      "--------Epoch 3 (9 total)--------\n",
      "Val loss = 0.0709, Zero-temp loss = 0.0712\n",
      "\n",
      "Stopping temp = 0.3162 at epoch 3\n",
      "\n",
      "Starting training with temp = 0.1778\n",
      "\n",
      "--------Epoch 1 (10 total)--------\n",
      "Val loss = 0.0694, Zero-temp loss = 0.0696\n",
      "\n",
      "--------Epoch 2 (11 total)--------\n",
      "Val loss = 0.0706, Zero-temp loss = 0.0707\n",
      "\n",
      "--------Epoch 3 (12 total)--------\n",
      "Val loss = 0.0688, Zero-temp loss = 0.0690\n",
      "\n",
      "Stopping temp = 0.1778 at epoch 3\n",
      "\n",
      "Starting training with temp = 0.1000\n",
      "\n",
      "--------Epoch 1 (13 total)--------\n",
      "Val loss = 0.0677, Zero-temp loss = 0.0677\n",
      "\n",
      "--------Epoch 2 (14 total)--------\n",
      "Val loss = 0.0682, Zero-temp loss = 0.0682\n",
      "\n",
      "--------Epoch 3 (15 total)--------\n",
      "Val loss = 0.0701, Zero-temp loss = 0.0702\n",
      "\n",
      "Stopping temp = 0.1000 at epoch 3\n",
      "\n",
      " Running full\n",
      "Starting training with temp = 1.0000\n",
      "\n",
      "--------Epoch 1 (1 total)--------\n",
      "Val loss = 1.2771, Zero-temp loss = 1.2464\n",
      "\n",
      "--------Epoch 2 (2 total)--------\n",
      "Val loss = 0.3402, Zero-temp loss = 0.3129\n",
      "\n",
      "--------Epoch 3 (3 total)--------\n",
      "Val loss = 0.2833, Zero-temp loss = 0.2775\n",
      "\n",
      "Stopping temp = 1.0000 at epoch 3\n",
      "\n",
      "Starting training with temp = 0.5623\n",
      "\n",
      "--------Epoch 1 (4 total)--------\n",
      "Val loss = 0.2514, Zero-temp loss = 0.2498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdfs_happy = GreedyDynamicSelection(selecting_net, predicting_net, mask_layer_happy)\n",
    "print(' Running Happy')\n",
    "gdfs_happy.fit(\n",
    "    happy_train_loader,\n",
    "    happy_val_loader,\n",
    "    lr=1e-3,\n",
    "    nepochs=3,\n",
    "    max_features=20,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    verbose=True)\n",
    "\n",
    "gdfs_dep = GreedyDynamicSelection(selecting_net, predicting_net, mask_layer_dep)\n",
    "print(' Running depressed')\n",
    "gdfs_dep.fit(\n",
    "    depressed_train_loader,\n",
    "    depressed_val_loader,\n",
    "    lr=1e-3,\n",
    "    nepochs=3,\n",
    "    max_features=20,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    verbose=True)\n",
    "\n",
    "gdfs_full = GreedyDynamicSelection(selecting_net, predicting_net, mask_layer_full)\n",
    "print(' Running full')\n",
    "gdfs_full.fit(\n",
    "    full_train_loader,\n",
    "    full_val_loader,\n",
    "    lr=1e-3,\n",
    "    nepochs=3,\n",
    "    max_features=20,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isak predict proba tensor([[0.7229, 0.0810],\n",
      "        [0.7783, 0.0568],\n",
      "        [0.7822, 0.0397],\n",
      "        ...,\n",
      "        [0.6806, 0.1380],\n",
      "        [0.7320, 0.0788],\n",
      "        [0.7746, 0.0448]], grad_fn=<SigmoidBackward0>)\n",
      "\n",
      "happy probs np: [[0.72293323 0.08100478]\n",
      " [0.77834624 0.0568247 ]\n",
      " [0.7822474  0.03968071]\n",
      " ...\n",
      " [0.6806005  0.13795824]\n",
      " [0.7319756  0.07881784]\n",
      " [0.77462614 0.04481372]]\n",
      "\n",
      "correct : [[0.53218881 0.46781119]\n",
      " [0.56179624 0.43820376]\n",
      " [0.64842525 0.35157475]\n",
      " ...\n",
      " [0.50926145 0.49073855]\n",
      " [0.53643343 0.46356657]\n",
      " [0.54613614 0.45386386]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x:\\Anaconda\\envs\\367A\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "happy_predictions,happy_mask,happy_m   = gdfs_happy.forward(X_val_tensor_happy,max_features =20)\n",
    "dep_predictions,dep_mask,dep_m   = gdfs_dep.forward(X_val_tensor_dep,max_features =20)\n",
    "full_predictions,full_mask,full_m   = gdfs_full.forward(X_val_tensor_full,max_features =20)\n",
    "\n",
    "happy_probabilities = torch.sigmoid(happy_predictions)\n",
    "happy_predicted_classes = torch.argmax(happy_predictions, dim=1)\n",
    "happy_binary_predictions = (happy_probabilities >= 0.5).float()\n",
    "\n",
    "# print(f' Happy preds: {happy_predictions}')\n",
    "print(f\"isak predict proba {happy_probabilities}\")\n",
    "# print(happy_predicted_classes)\n",
    "\n",
    "happy_probs_np = happy_probabilities.detach().cpu().numpy()\n",
    "\n",
    "print( )\n",
    "\n",
    "print(f\"happy probs np: {happy_probs_np}\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "dep_probabilities = torch.sigmoid(dep_predictions)\n",
    "dep_predicted_classes = torch.argmax(dep_predictions, dim=1)\n",
    "dep_binary_predictions = (dep_probabilities >= 0.5).float()\n",
    "\n",
    "# print(f' dep preds: {dep_predictions}')\n",
    "# print(dep_probabilities)\n",
    "# print(dep_predicted_classes)\n",
    "\n",
    "# print( )\n",
    "\n",
    "full_probabilities = torch.sigmoid(full_predictions)\n",
    "full_predicted_classes = torch.argmax(full_predictions, dim=1)\n",
    "full_binary_predictions = (full_probabilities >= 0.5).float()\n",
    "\n",
    "# print(f' Full preds: {full_predictions}')\n",
    "# print(full_probabilities)\n",
    "# print(full_predicted_classes)\n",
    "\n",
    "# TODO: Slette dette?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyProbaWrapper:\n",
    "    def __init__(self, greedy_model, max_features=20):\n",
    "        # force the underlying model (and its helper device) to CPU\n",
    "        self.greedy = greedy_model.cpu()\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # turn X into a plain numpy array\n",
    "        X_arr = X.values if hasattr(X, \"values\") else X\n",
    "        # build a CPU tensor (so m = zeros(...) also lands on CPU)\n",
    "        X_t   = torch.tensor(X_arr, dtype=torch.float32)  \n",
    "        # forward returns (logits, mask, m)\n",
    "        logits, mask, m = self.greedy(X_t, self.max_features)\n",
    "        # sigmoid  probabilities\n",
    "        probs = torch.sigmoid(logits)\n",
    "        # back to numpy\n",
    "        return probs.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "greedy_wrapped = GreedyProbaWrapper(gdfs_full, max_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total amount of candidates:  1\n",
      "Best parameters so far: 200, 0.7, 15, 4, 2, with accuracy: 0.9666\n",
      "Remaining candidates: 0/1\n",
      "The best performing model with an accuracy score: 0.9666, had these parameters:\n",
      "N_final: 200\n",
      "m: 0.7\n",
      "max_depth: 15\n",
      "min_samples_leaf: 4\n",
      "min_samples_split: 2\n",
      "The total amount of candidates:  1\n",
      "Best parameters so far: 200, 0.7, 15, 4, 2, with accuracy: 0.8183\n",
      "Remaining candidates: 0/1\n",
      "The best performing model with an accuracy score: 0.8183, had these parameters:\n",
      "N_final: 200\n",
      "m: 0.7\n",
      "max_depth: 15\n",
      "min_samples_leaf: 4\n",
      "min_samples_split: 2\n",
      "The total amount of candidates:  1\n",
      "Best parameters so far: 200, 0.7, 15, 4, 2, with accuracy: 0.8829\n",
      "Remaining candidates: 0/1\n",
      "The best performing model with an accuracy score: 0.8829, had these parameters:\n",
      "N_final: 200\n",
      "m: 0.7\n",
      "max_depth: 15\n",
      "min_samples_leaf: 4\n",
      "min_samples_split: 2\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best Hyperparameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best Hyperparameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best Hyperparameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "irf, _, _ = hypertuned_model(training_data)\n",
    "dirf, _, _ = hypertuned_model(df_special_depr)\n",
    "hirf, _, _ = hypertuned_model(df_special_non_depr)\n",
    "\n",
    "combined_base_learners = {\n",
    "    \"base model\": Random_forest(seed, X_train, y_train),\n",
    "    \"base model depr\": Random_forest(seed, X_special_depr, y_special_depr),\n",
    "    \"base model happy\": Random_forest(seed, X_special_non_depr, y_special_non_depr),\n",
    "    \"improved random forest\": irf,\n",
    "    \"improved random forest depr\": dirf,\n",
    "    \"improved random forest happy\": hirf,\n",
    "    \"greedy\": gdfs_full,\n",
    "    \"greedy depressed\": gdfs_dep,\n",
    "    \"greedy happy\": gdfs_happy\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import selection_net_module\n",
    "\n",
    "# re-read the .py file and replace the module in memory\n",
    "importlib.reload(selection_net_module)\n",
    "\n",
    "from selection_net_module import SelectionNetwork, ensemble_forward, get_base_predictions, tune_selection_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning selection network...\n",
      "Best k=1 with validation accuracy=0.9351\n"
     ]
    }
   ],
   "source": [
    "# Default selection net (Elias's paper alone)\n",
    "\n",
    "\n",
    "ks = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "feature_dim = sn_X_train.shape[1]\n",
    "\n",
    "epoch = 15\n",
    "\n",
    "\n",
    "best_k_special, best_acc, best_state_special, results, losses, accs = tune_selection_net(\n",
    "    specialized_learners, sn_X_train, sn_y_train, sn_X_val, sn_y_val,\n",
    "    ks, feature_dim, num_epochs=epoch, epsilon=0.1, m=1000, batch_size=512, plot=True\n",
    ")\n",
    "\n",
    "print(f\"Best k={best_k_special} with validation accuracy={best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning selection network...\n",
      "Best k=1 with validation accuracy=0.8186\n"
     ]
    }
   ],
   "source": [
    "# Combined selection net where we use magnus paper, isaks paper and our base model as base learners -> 9 base learners\n",
    "\n",
    "ks = [1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "feature_dim = sn_X_train.shape[1]\n",
    "epoch = 15\n",
    "\n",
    "best_k_combined, best_acc, best_state_combined, results, losses, accs = tune_selection_net(\n",
    "    combined_base_learners, sn_X_train, sn_y_train, sn_X_val, sn_y_val,\n",
    "    ks, feature_dim, num_epochs=epoch, epsilon=0.1, m=1000, batch_size=512,plot= True\n",
    ")\n",
    "\n",
    "print(f\"Best k={best_k_combined} with validation accuracy={best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot all the different models accuracy on validations\n",
    "\n",
    "### Go further with the best one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelNet-Special        Val accuracy: 0.9351\n",
      "SelNet-Combined       Val accuracy: 0.8186\n",
      "Magnus Imp. RF        Val accuracy: 0.8123\n",
      "Isak Greedy           Val accuracy: 0.8186\n",
      "Base RF               Val accuracy: 0.8183\n",
      "\n",
      "Best performing model: SelNet-Special with accuracy 0.9351\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from selection_net_module import SelectionNetwork, get_base_predictions, ensemble_forward\n",
    "\n",
    "# 1) Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2) Helper to evaluate a tuned SelectionNet\n",
    "def evaluate_selection_net(net_state, base_learners, best_k,\n",
    "                           X_val, y_val, feature_dim, epsilon=0.1, m=100):\n",
    "    # rebuild & load\n",
    "    net = SelectionNetwork(input_size=feature_dim, num_learners=len(base_learners))\n",
    "    net.load_state_dict(net_state)\n",
    "    net.to(device).eval()\n",
    "    # prepare tensors\n",
    "    X_t   = torch.tensor(X_val.values if hasattr(X_val, 'values') else X_val,\n",
    "                         dtype=torch.float32, device=device)\n",
    "    bp_np = get_base_predictions(X_val, base_learners)\n",
    "    bp_t  = torch.tensor(bp_np, dtype=torch.float32, device=device)\n",
    "    # forward & accuracy\n",
    "    with torch.no_grad():\n",
    "        probs = ensemble_forward(net, X_t, bp_t, best_k, epsilon, m)\n",
    "        preds = probs.argmax(dim=1).cpu().numpy()\n",
    "    return accuracy_score(y_val, preds)\n",
    "\n",
    "# 3) Torchbased accuracy metric for greedy model\n",
    "def torch_acc(preds, labels):\n",
    "    if preds.ndim > 1:\n",
    "        preds = preds.argmax(dim=1)\n",
    "    return (preds == labels).float().mean()\n",
    "\n",
    "# 4) Build a DataLoader for greedymodel evaluation\n",
    "Xvl = sn_X_val.values if hasattr(sn_X_val, 'values') else np.asarray(sn_X_val)\n",
    "yvl = sn_y_val.values if hasattr(sn_y_val, 'values') else np.asarray(sn_y_val)\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(Xvl, dtype=torch.float32),\n",
    "    torch.tensor(yvl, dtype=torch.long)\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "# 5) List of models to test\n",
    "models_to_test = [\n",
    "    (\"SelNet-Special\",   \"Selection\",      best_state_special, specialized_learners,   best_k_special,   0.1, 1000),\n",
    "    (\"SelNet-Combined\",  \"Selection\",      best_state_combined,combined_base_learners,best_k_combined,  0.1, 1000),\n",
    "    (\"Magnus Imp. RF\",   \"Random Forest\",  irf,                None,                  None,            None, None),\n",
    "    (\"Isak Greedy\",      \"Neural Network\", gdfs_full,          None,                  None,            None, None),\n",
    "    (\"Base RF\",          \"Random Forest\",  base_model,         None,                  None,            None, None),\n",
    "]\n",
    "\n",
    "#TODO: Er gdfs_full den beste tunet modellen? Samme med base model og irf?\n",
    "\n",
    "# 6) Run evaluation loop\n",
    "results = {}\n",
    "feature_dim = sn_X_train.shape[1]\n",
    "\n",
    "for name, mtype, obj, learners, k, eps, m in models_to_test:\n",
    "    if mtype == \"Selection\":\n",
    "        acc = evaluate_selection_net(\n",
    "            obj, learners, k,\n",
    "            sn_X_val, sn_y_val,\n",
    "            feature_dim, epsilon=eps, m=m\n",
    "        )\n",
    "    elif mtype == \"Random Forest\":\n",
    "        acc = accuracy_score(sn_y_val, obj.predict(sn_X_val))\n",
    "    elif mtype == \"Neural Network\":\n",
    "        acc = obj.evaluate(\n",
    "            loader=val_loader,\n",
    "            max_features=20,\n",
    "            metric=torch_acc,\n",
    "            argmax=True\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type {mtype}\")\n",
    "    results[name] = acc\n",
    "\n",
    "# 7) Print summary\n",
    "for name, acc in results.items():\n",
    "    print(f\"{name:20s}  Val accuracy: {acc:.4f}\")\n",
    "\n",
    "best_name = max(results, key=results.get)\n",
    "print(f\"\\nBest performing model: {best_name} with accuracy {results[best_name]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "367A",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
