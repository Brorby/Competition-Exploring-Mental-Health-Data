{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import matplotlib\n",
    "import tokenizers\n",
    "import datasets\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "Here we explore each feature column and comment on how we are going to preprocess each one and what we might consider doing in the future. We have a total of 140700 samples. The last feature is the label depression which we will train on.\n",
    "\n",
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "data_file = 'data/train.csv'\n",
    "test_file = 'data/test.csv'\n",
    "\n",
    "data = pd.read_csv(data_file)\n",
    "test = pd.read_csv(test_file)\n",
    "\n",
    "print(f\"Samples in training set {data.shape[0]}\")\n",
    "print(f\"Samples in test set: {test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# id Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print examples of the 'id' column\n",
    "print(\"Examples of 'id' column:\")\n",
    "print(data[\"id\"].head())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"We have \" ,data[\"id\"].count(),\" amount of samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop the id column since there is no correlation between this and the label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each unique name\n",
    "name_counts = data[\"Name\"].value_counts()\n",
    "\n",
    "# Print results\n",
    "print(\"Unique names sorted by count (most to least):\")\n",
    "for name, count in name_counts.items():\n",
    "    print(f\"{name}: {count}\")\n",
    "\n",
    "print(f\"Total unique names: {len(name_counts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thought about dropping the name column since we thought that this may not have any correlation to the label. But we realised that there might be a correlation, so we decided to keep it. The correlation being that for example having a \"unattractive\" name can affect your life in a bad way. \n",
    "\n",
    "\n",
    "For pre-processing this we will to give a value of how unique it is in the dataset, so for now we change the name with how frequent it is.\n",
    "\n",
    "\n",
    "Things to consider:\n",
    "\n",
    "Find a way to \"rate\" each name instead of how frequent the name is. Impute or change wrong names to missing name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print examples of the 'Name' column\n",
    "print(\"Examples of 'Name' column:\")\n",
    "print(data[\"Gender\"].head())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of unique genders:\", data[\"Gender\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just encode male to 1 and female to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Examples of 'Age' column:\")\n",
    "print(data[\"Age\"].head())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of unique ages:\", data[\"Age\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only change the number from float to integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Examples of 'City' column:\")\n",
    "print(data[\"City\"].head())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of unique cities:\", data[\"Age\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each unique city\n",
    "city_counts = data[\"City\"].value_counts()\n",
    "\n",
    "# Find first occurrence index for each unique city\n",
    "first_occurrence = data.groupby(\"City\").apply(lambda x: x.index[0])\n",
    "\n",
    "# Sort cities by count in descending order\n",
    "sorted_cities = city_counts.index  # Cities sorted by count (default sorting from most to least)\n",
    "\n",
    "count = 0\n",
    "# Print results\n",
    "print(\"Unique cities sorted by count (most to least):\")\n",
    "for city in sorted_cities:\n",
    "    if city_counts[city] > 0:\n",
    "        count += 1\n",
    "        print(f\"{city}: First Index = {first_occurrence[city]}, Count = {city_counts[city]}\")\n",
    "\n",
    "print(f\"Total unique cities: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found population, density, literacy and sex ratio for each major city in our dataset. We then merged this in our data and removed city column. For the minor cities or the wrongly written cities we took the average of the other columns.\n",
    "\n",
    "\n",
    "To consider:\n",
    "\n",
    "We should consider adjusting the imputing of the minor and wrongly written cities to for example lower than average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Professional or Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Examples of 'Working Professional or Student' column:\")\n",
    "print(data[\"Working Professional or Student\"].head())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of unique Working Professional or Student:\", data[\"Working Professional or Student\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this column we will change \"working professional\" to 0 and \"student\" to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Examples of 'Profession' column:\")\n",
    "print(data[\"Profession\"].head())\n",
    "print(\"\")\n",
    "print(\"NaN is student:\")\n",
    "print(data[\"Profession\"][2])\n",
    "print(data[\"Working Professional or Student\"][2])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of unique Profession:\", data[\"Profession\"].nunique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are occurences of NaN in this column, this happens when the sample is a student. We find it reasonable to insert \"Student\" in those slots. there are also occurences of NaN on samples that are not students, here we will insert \"Missing Profession\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Academic Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Examples of 'Academic pressure' column:\")\n",
    "print(data[\"Academic Pressure\"].head())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of unique Academic pressures:\", data[\"Academic Pressure\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the \"Academic Pressure\" column we want to replace NaN with zeros.\n",
    "We are aslo considering merging this feature with work pressure, since they complete eachother. but for now we will have it like this and maybe change it for improving the model later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Examples of 'Work Pressure' column:\")\n",
    "print(data[\"Work Pressure\"].head())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of unique Work Pressure:\", data[\"Work Pressure\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that a lower number means less work pressure, therefore We will change Nan to 0 because it is the students that has NaN on Work Pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CGPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Examples of 'CGPA' column:\")\n",
    "print(data[\"CGPA\"].head())\n",
    "print(data[\"CGPA\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider changing the NaN slots to the average of the dataset, which is 7.66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study Satisfaction and Job Satisfaction (Satisfaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Examples of 'Study Satisfaction' and 'Job Satisfaction' column:\")\n",
    "print(data[\"Study Satisfaction\"].head())\n",
    "print(data[\"Job Satisfaction\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can tell that when there is missing a value in the study satisfaction column, there is a value in the same sample but on the job satisfaction problem. these two columns complete eachother, so we will combine these two columns into one \"satisfaction\" column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_counts = data[\"Sleep Duration\"].value_counts()\n",
    "\n",
    "print(\"Unique sleep sorted by count (most to least):\")\n",
    "for e, (duration, count) in enumerate(sleep_counts.items()):\n",
    "    print(duration, count)\n",
    "    if e == 6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one looks a bit tricky. since there are so few occurnces of other than the four most common inputs in this feature, we will change the numbers to a scale from 1 to 4, where 1 is \"less than 5 hours\" all the way to 4 which is \"more than 8 hours\". all the others will be set to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dietary Habits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diet_counts = data[\"Dietary Habits\"].value_counts()\n",
    "\n",
    "print(\"Unique sleep sorted by count (most to least):\")\n",
    "for e, (diet, count) in enumerate(diet_counts.items()):\n",
    "    print(diet, count)\n",
    "    if e == 6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Healthy -> 2\n",
    "* Moderate -> 1\n",
    "* Unhealthy -> 0\n",
    "* The rest -> 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_counts = data[\"Degree\"].value_counts()\n",
    "print(data[\"Degree\"].nunique())\n",
    "\n",
    "print(\"Unique sleep sorted by count (most to least):\")\n",
    "for e, (degree, count) in enumerate(degree_counts.items()):\n",
    "    print(degree, count)\n",
    "    if e == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is particularly difficult because there are 115 unique degrees and there are not just a few degrees that covers the majority of the dataset either as the case is in the dietary habit feature. Our approach here is to somehow categorize the different degrees into bachelor, master, doctrine etc. and then give each of them a number from -1 to 4 based on the rank of the degree, going from \"other\" (which is the case where we can't define what degree it is) to professional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have you ever had suicidal thoughts ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Examples of 'Have you ever had suicidal thoughts ?' column:\")\n",
    "print(data[\"Have you ever had suicidal thoughts ?\"].head())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of unique Have you ever had suicidal thoughts ?:\", data[\"Have you ever had suicidal thoughts ?\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also change this column to binary no / yes to 0 / 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work Study Hours (No prerocessing needed here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_counts = data[\"Work/Study Hours\"].value_counts()\n",
    "print(data[\"Work/Study Hours\"].nunique())\n",
    "\n",
    "print(\"Unique Work/Study Hours sorted by count (most to least):\")\n",
    "for e, (ws, count) in enumerate(ws_counts.items()):\n",
    "    print(ws, count)\n",
    "    if e == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finans_count = data[\"Financial Stress\"].value_counts()\n",
    "print(data[\"Financial Stress\"].nunique())\n",
    "\n",
    "print(\"Unique sleep sorted by count (most to least):\")\n",
    "for e, (fn, count) in enumerate(finans_count.items()):\n",
    "    print(fn, count)\n",
    "    if e == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has a few missing values, we will impute the mean in these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Family History of Mental Illness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Examples of 'Family History of Mental Illness' column:\")\n",
    "print(data[\"Family History of Mental Illness\"].head())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of unique Family History of Mental Illnesses:\", data[\"Family History of Mental Illness\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this column we will change yes and no to 1 and 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depression - The label\n",
    "Plotting the distribution of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts = data['Depression'].value_counts()\n",
    "target_counts.plot(kind='bar', color=['pink', 'black'])\n",
    "plt.xticks(ticks=[0, 1], labels=['Not Depressed', 'Depressed'], rotation=0)\n",
    "plt.title('Distribution of Not Depressed vs Depressed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can tell that there is a significant imbalance between depressed and not depressed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the Id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropId(data):\n",
    "    data = data.drop(columns=[\"id\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the name column, Replacing each name with its frequency in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_freq(data):\n",
    "    data[\"Name\"] = data[\"Name\"].map(data[\"Name\"].value_counts())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding gender column 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_encode(data):\n",
    "    data[\"Gender\"] = data[\"Gender\"].map({\"Female\": 0, \"Male\": 1 })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age, changing the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_integer(data):\n",
    "    data[\"Age\"] = data[\"Age\"].astype(int)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Academic Preessure, inserting 0 in the empty slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def academic_pressure(data):\n",
    "    data[\"Academic Pressure\"] = np.where(\n",
    "        data[\"Academic Pressure\"].isnull(), 0,\n",
    "        data[\"Academic Pressure\"]\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Family History of Mental Illnes. -> 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def family_mental_illness(data):\n",
    "    data[\"Family History of Mental Illness\"] = np.where(\n",
    "        data[\"Family History of Mental Illness\"] == \"No\", 0,\n",
    "        np.where(\n",
    "            data[\"Family History of Mental Illness\"] == \"Yes\", 1,\n",
    "            data[\"Family History of Mental Illness\"]\n",
    "        )\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you ever had suicidal thoughts? -> 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suicidal_thoughts(data):\n",
    "    data[\"Have you ever had suicidal thoughts ?\"] = np.where(\n",
    "        data[\"Have you ever had suicidal thoughts ?\"] == \"No\", 0,\n",
    "        np.where(\n",
    "            data[\"Have you ever had suicidal thoughts ?\"] == \"Yes\", 1,\n",
    "            data[\"Have you ever had suicidal thoughts ?\"]\n",
    "        )\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Financial stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def financial_stress(data):\n",
    "    data[\"Financial Stress\"] = np.where(\n",
    "        data[\"Financial Stress\"].isnull(), round(data[\"Financial Stress\"].mean()), data[\"Financial Stress\"]\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working Professional or Student. -> 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def working_or_student(data):\n",
    "    data[\"Working Professional or Student\"] = np.where(\n",
    "        data[\"Working Professional or Student\"] == \"Working Professional\", 0,\n",
    "        np.where(\n",
    "            data[\"Working Professional or Student\"] == \"Student\", 1,\n",
    "            data[\"Working Professional or Student\"]\n",
    "        )\n",
    "    )       \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profession. Inserting Student in the empty slots if the sample is a student and missing profession if it is empty and the sample is not a student. also inserting \"Missing Profession\" in the professions that has less than 10 occurrences in the dataset because the majority of these are wrong information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profession(data):\n",
    "    # Count occurrences of each profession\n",
    "    profession_counts = data[\"Profession\"].value_counts()\n",
    "    \n",
    "    # Update the \"Profession\" column\n",
    "    data[\"Profession\"] = np.where(\n",
    "        data[\"Profession\"].isnull() & (data[\"Working Professional or Student\"] == 1),\n",
    "        \"Student\",\n",
    "        np.where(\n",
    "            data[\"Profession\"].isnull() & (data[\"Working Professional or Student\"] == 0), \n",
    "            \"Missing Profession\", \n",
    "            np.where(\n",
    "                data[\"Profession\"].map(profession_counts) < 6,  # If occurrence < 10, set to \"Missing Profession\"\n",
    "                \"Missing Profession\",\n",
    "                data[\"Profession\"]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorizing Professions and giving each category a number. the number is a rating from 0 to 10 on how happy people are in given categories. We used chatGPT to rate them based on different logical points. see professions_mapping dictionary for description of the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_profession(profession):\n",
    "    if pd.isna(profession) or \"missing\" in profession.lower():\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    profession = profession.lower().strip()\n",
    "\n",
    "    # Technology\n",
    "    tech_keywords = [\"software\", \"data scientist\", \"ux/ui\", \"developer\", \"engineer\"]\n",
    "    if any(keyword in profession for keyword in tech_keywords):\n",
    "        return \"Technology\"\n",
    "\n",
    "    # Finance\n",
    "    finance_keywords = [\"accountant\", \"financial\", \"investment\", \"banker\", \"analyst\"]\n",
    "    if any(keyword in profession for keyword in finance_keywords):\n",
    "        return \"Finance\"\n",
    "\n",
    "    # Healthcare\n",
    "    healthcare_keywords = [\"doctor\", \"pharmacist\", \"dentist\", \"nurse\"]\n",
    "    if any(keyword in profession for keyword in healthcare_keywords):\n",
    "        return \"Healthcare\"\n",
    "\n",
    "    # Education\n",
    "    education_keywords = [\"teacher\", \"professor\", \"educational\"]\n",
    "    if any(keyword in profession for keyword in education_keywords):\n",
    "        return \"Education\"\n",
    "\n",
    "    # Engineering\n",
    "    engineering_keywords = [\"civil engineer\", \"mechanical engineer\", \"architect\"]\n",
    "    if any(keyword in profession for keyword in engineering_keywords):\n",
    "        return \"Engineering\"\n",
    "\n",
    "    # Marketing & Sales\n",
    "    marketing_keywords = [\"marketing\", \"sales\", \"digital marketer\", \"content writer\"]\n",
    "    if any(keyword in profession for keyword in marketing_keywords):\n",
    "        return \"Marketing/Sales\"\n",
    "\n",
    "    # Trade & Manual Work\n",
    "    trade_keywords = [\"electrician\", \"plumber\", \"chef\", \"mechanic\"]\n",
    "    if any(keyword in profession for keyword in trade_keywords):\n",
    "        return \"Trade\"\n",
    "\n",
    "    # Legal\n",
    "    legal_keywords = [\"lawyer\", \"judge\", \"legal\"]\n",
    "    if any(keyword in profession for keyword in legal_keywords):\n",
    "        return \"Legal\"\n",
    "\n",
    "    # Consulting\n",
    "    consulting_keywords = [\"consultant\", \"business analyst\"]\n",
    "    if any(keyword in profession for keyword in consulting_keywords):\n",
    "        return \"Consulting\"\n",
    "\n",
    "    # Other / Unknown\n",
    "    return \"Other\"\n",
    "\n",
    "def Prefession_categorization(data):\n",
    "    # Apply categorization\n",
    "    data[\"Profession_Category\"] = data[\"Profession\"].apply(categorize_profession)\n",
    "\n",
    "    # Encoding Options\n",
    "\n",
    "    ## Option 1: Ordinal Encoding (useful if there's a natural order)\n",
    "    profession_mapping = {\n",
    "    \"Unknown\": 0,           # No data on job satisfaction\n",
    "    \"Other\": 4,             # Mixed bag, depends on the job\n",
    "    \"Trade\": 6,             # Skilled trades often provide satisfaction, but physically demanding\n",
    "    \"Marketing/Sales\": 5,   # High variability, stressful but rewarding\n",
    "    \"Consulting\": 5,        # High pay but often high stress & long hours\n",
    "    \"Education\": 7,         # Fulfilling, but pay can be low and stress can be high\n",
    "    \"Finance\": 6,           # High pay, but long hours and stress\n",
    "    \"Engineering\": 7,       # Good pay, problem-solving, but deadlines can be stressful\n",
    "    \"Healthcare\": 6,        # Rewarding but high stress and burnout risk\n",
    "    \"Legal\": 5,             # High salary, but stressful and demanding\n",
    "    \"Technology\": 8         # High salary, flexibility, remote work options\n",
    "}\n",
    "\n",
    "\n",
    "    data[\"Profession_Encoded\"] = data[\"Profession_Category\"].map(profession_mapping)\n",
    "\n",
    "    data[\"Profession\"] = data[\"Profession_Encoded\"]\n",
    "\n",
    "    data = data.drop(columns=[\"Profession_Encoded\", \"Profession_Category\"])\n",
    "    return data\n",
    "    ## Option 2: One-Hot Encoding (better for categorical data)\n",
    "    # data = pd.get_dummies(data, columns=[\"Profession_Category\"], prefix=\"Profession\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def work_pressure(data):\n",
    "    data[\"Work Pressure\"] = np.where(\n",
    "        data[\"Work Pressure\"].isnull(), 0.0, data[\"Work Pressure\"]\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CGPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cgpa(data):\n",
    "    data[\"CGPA\"] = np.where(\n",
    "        data[\"CGPA\"].isnull(), round(data[\"CGPA\"].mean(), 2), data[\"CGPA\"]\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study- and Job Satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def satisfaction(data):\n",
    "    satisfaction = np.where(\n",
    "        data[\"Study Satisfaction\"].notnull(), data[\"Study Satisfaction\"], np.where(\n",
    "            data[\"Job Satisfaction\"].notnull(), data[\"Job Satisfaction\"], 0\n",
    "        )\n",
    "    )\n",
    "    data[\"Job Satisfaction\"] = satisfaction\n",
    "    data = data.drop(columns=\"Study Satisfaction\")\n",
    "    data.rename(columns={\"Job Satisfaction\": \"Satisfaction\"}, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sleep Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_duration(data):\n",
    "    data[\"Sleep Duration\"] = np.where(\n",
    "        data[\"Sleep Duration\"] == \"Less than 5 hours\", 1, np.where(\n",
    "            data[\"Sleep Duration\"] == \"5-6 hours\", 2, np.where(\n",
    "                data[\"Sleep Duration\"] == \"7-8 hours\", 3, np.where(\n",
    "                    data[\"Sleep Duration\"] == \"More than 8 hours\", 4, 1\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dietary Habits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diet(data):\n",
    "    data[\"Dietary Habits\"] = np.where(\n",
    "        data[\"Dietary Habits\"] == \"Healthy\", 2, np.where(\n",
    "            data[\"Dietary Habits\"] == \"Moderate\", 1, np.where(\n",
    "                data[\"Dietary Habits\"] == \"Unhealthy\", 0, 1\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_degree(degree):\n",
    "\n",
    "    \"\"\"\n",
    "    Categorizes the differnet degrees into Schoo, Bachelors, Masters, Doctrate, professional and other\n",
    "    \"\"\"\n",
    "    \n",
    "    degree = str(degree).strip().lower()\n",
    "\n",
    "    # School-Level\n",
    "    if \"class 11\" in degree or \"class 12\" in degree:\n",
    "        return \"School\"\n",
    "\n",
    "    # Bachelor's Degrees\n",
    "    bachelor_keywords = [\"b.\", \"b \", \"bachelor\", \"bcom\", \"bsc\", \"btech\", \"be\", \"bba\", \"bca\", \"ba\", \"b.ed\", \"b.arch\"]\n",
    "    if any(keyword in degree for keyword in bachelor_keywords):\n",
    "        return \"Bachelors\"\n",
    "\n",
    "    # Master's Degrees\n",
    "    master_keywords = [\"m.\", \"m \", \"master\", \"mba\", \"mcom\", \"msc\", \"mtech\", \"me\", \"mca\", \"m.ed\", \"mpharm\", \"m.arch\"]\n",
    "    if any(keyword in degree for keyword in master_keywords):\n",
    "        return \"Masters\"\n",
    "\n",
    "    # Doctoral Degrees\n",
    "    if \"phd\" in degree:\n",
    "        return \"Doctorate\"\n",
    "\n",
    "    # Professional Degrees\n",
    "    professional_keywords = [\"mbbs\", \"md\", \"llb\", \"llm\"]\n",
    "    if any(keyword in degree for keyword in professional_keywords):\n",
    "        return \"Professional\"\n",
    "\n",
    "    # Unknown / Noisy Data\n",
    "    return \"Other\"\n",
    "\n",
    "def degree(data):\n",
    "    \n",
    "    # Apply categorization\n",
    "    data[\"Degree\"] = data[\"Degree\"].apply(categorize_degree)\n",
    "\n",
    "\n",
    "    # Define an ordinal mapping\n",
    "    degree_mapping = {\n",
    "        \"School\": 1,\n",
    "        \"Bachelors\": 2,\n",
    "        \"Masters\": 3,\n",
    "        \"Professional\": 4,\n",
    "        \"Doctorate\": 5,\n",
    "        \"Other\": 0  # Keep 'Other' at the highest level or remove it depending on the approach\n",
    "    }\n",
    "\n",
    "    # Apply mapping\n",
    "    data[\"Degree\"] = data[\"Degree\"].map(degree_mapping)\n",
    "\n",
    "    return data \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load city data\n",
    "city_data_path = \"./Data/indian_cities_data.csv\"\n",
    "df_cities = pd.read_csv(city_data_path)\n",
    "\n",
    "def city_one_hot(data):\n",
    "    # Find the index of the \"City\" column\n",
    "    city_index = data.columns.get_loc(\"City\")\n",
    "\n",
    "    # Merge the data with city information, excluding \"Main Language\"\n",
    "    merged_data = data.merge(df_cities.drop(columns=[\"Main Language\"]), on=\"City\", how=\"left\")\n",
    "\n",
    "    # Calculate the mean values for missing cities\n",
    "    mean_values = merged_data[[\"Population\", \"Density (per km²)\", \"Literacy Rate (%)\", \"Sex Ratio\"]].mean()\n",
    "\n",
    "    # Replace NaN values with the mean of the respective column\n",
    "    merged_data.fillna(mean_values, inplace=True)\n",
    "\n",
    "    # Drop the original \"City\" column\n",
    "    merged_data.drop(columns=[\"City\"], inplace=True)\n",
    "\n",
    "    # Reorder columns to place new city-related columns where \"City\" was\n",
    "    city_columns = [\"Population\", \"Density (per km²)\", \"Literacy Rate (%)\", \"Sex Ratio\"]\n",
    "    cols = merged_data.columns.tolist()\n",
    "\n",
    "    # Move the new city-related columns to the correct index\n",
    "    for col in reversed(city_columns):\n",
    "        cols.insert(city_index, cols.pop(cols.index(col)))\n",
    "\n",
    "    # Reorder the dataframe\n",
    "    merged_data = merged_data[cols]\n",
    "\n",
    "    return merged_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(data):\n",
    "\n",
    "    data = dropId(data)\n",
    "    data = name_freq(data)\n",
    "    data = gender_encode(data)\n",
    "    data = age_integer(data)\n",
    "    data = academic_pressure(data)\n",
    "    data = family_mental_illness(data)\n",
    "    data = suicidal_thoughts(data)\n",
    "    data = working_or_student(data)\n",
    "    data = profession(data)\n",
    "    data = work_pressure(data)\n",
    "    data = cgpa(data)\n",
    "    data = satisfaction(data)\n",
    "    data = sleep_duration(data)\n",
    "    data = diet(data)\n",
    "    data = degree(data)\n",
    "    data = Prefession_categorization(data)\n",
    "    data = city_one_hot(data)\n",
    "    data = financial_stress(data)\n",
    "    return data\n",
    "\n",
    "training_data = pre_processing(data)\n",
    "test_data = pre_processing(test)\n",
    "test_data.to_csv(\"test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is The function for Normalizing (0-1) the dataset. This is necessary or helpful if we want to use:\n",
    "\n",
    "| Learning Algorithm |\n",
    "|--------------------|\n",
    "| Logistic Regression |\n",
    "| SVM |\n",
    "| Neural Network - better |\n",
    "| K-Means |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(training_data, test_data, columns_to_scale):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(training_data[columns_to_scale])\n",
    "    training_data[columns_to_scale] = scaler.transform(training_data[columns_to_scale])\n",
    "    test_data[columns_to_scale] = scaler.transform(test_data[columns_to_scale])\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is The function for Standardising the dataset. This is necessary or helpful if we want to use:\n",
    "\n",
    "| Learning Algorithm |\n",
    "|--------------------|\n",
    "| Logistic Regression |\n",
    "| SVM - better |\n",
    "| Neural Network |\n",
    "| PCA |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_dataset(training_data, test_data, columns_to_scale):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(training_data[columns_to_scale])\n",
    "    training_data[columns_to_scale] = scaler.transform(training_data[columns_to_scale])\n",
    "    test_data[columns_to_scale] = scaler.transform(test_data[columns_to_scale])\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns that needs to be scaled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_scale = [\"Population\", \"Density (per km²)\", \"Literacy Rate (%)\", \"CGPA\", \"Work/Study Hours\"]\n",
    "\n",
    "def scale(method: str, training_data, test_data):\n",
    "    if method == \"Standardize\":\n",
    "        training_data, test_data = standardize_dataset(training_data, test_data, columns_to_scale)\n",
    "    else:\n",
    "        training_data, test_data = normalize_dataset(training_data, test_data, columns_to_scale)\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into Train and val, also extracting the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_data(seed=0, train_size=0.8, data=training_data):\n",
    "    np.random.seed(seed)\n",
    "    X = data.drop(columns=[\"Depression\"])\n",
    "    y = data[\"Depression\"]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=train_size, random_state=seed, stratify=y)\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out Random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test method to be used anytime you want to try your model. Prints the submission in \"submission.csv\" in the working directory if you want to submit, but also tests it on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data, X_val, y_val, test_or_val=\"val\"):\n",
    "    if test_or_val == \"test\":\n",
    "        predictions = model.predict(test_data)\n",
    "\n",
    "        submission = pd.DataFrame({\n",
    "            \"id\": test[\"id\"], \n",
    "            \"Depression\": predictions\n",
    "        })\n",
    "        submission.to_csv(\"submission.csv\", index=False)\n",
    "        print(\"test successfully executed, results in submission.csv\")\n",
    "    elif test_or_val == \"val\":\n",
    "        predictions = model.predict(X_val)\n",
    "        accuracy = accuracy_score(predictions, y_val)\n",
    "        print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_forest(seed, X_train, y_train):\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [10, 20, 30],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [200],\n",
    "        \"max_depth\": [20],\n",
    "        \"min_samples_split\": [5],\n",
    "        \"min_samples_leaf\": [1]\n",
    "    }\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    rf = RandomForestClassifier(random_state=seed)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=rf,\n",
    "        param_grid=param_grid,\n",
    "        cv=kf,\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1,  # Use all available CPU cores\n",
    "        verbose=1   # Shows progress\n",
    "    )\n",
    "\n",
    "    # No scaling required\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    \n",
    "    model = RandomForestClassifier(max_depth=best_params[\"max_depth\"], \n",
    "                            min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "                            min_samples_split=best_params[\"min_samples_split\"],\n",
    "                            n_estimators=best_params[\"n_estimators\"],\n",
    "                            class_weight=\"balanced\"\n",
    "                            )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "rf_training_data, rf_test_data = scale(\"Standardize\", training_data, test_data)\n",
    "X_train, X_val, y_train, y_val = split_training_data(0, data=rf_training_data)\n",
    "model = Random_forest(0, X_train, y_train)\n",
    "test_model(model, rf_test_data, X_val, y_val, \"val\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF367A",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
