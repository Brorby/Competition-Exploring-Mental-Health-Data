{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF367A Applied Machine Learning - Group project\n",
    "\n",
    "* Elias Hovdenes\n",
    "* Isak Pall Gestsson\n",
    "* Magnus Sponnich Brørby\n",
    "\n",
    "## Exploring Mental Health Data - Competition\n",
    "\n",
    "### Exploring the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import train_test_split,HalvingGridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler , OneHotEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i prepare data kombineres academic og work pressure til en kolonne pressure. i tillegg finner den kategoriske og numeriske features og lage en preprocessing pipeline for hver av de. for de numeriske verdiene så imputer den missing verdier med gjennomsnittet av den aktuelle kolonnen og så scaler dataene. For de kategoriske verdiene så imputer den de som intreffer mest og så one hot encoder de."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_data(dataset,preprocessor = None):\n",
    "\n",
    "    X = pd.DataFrame(dataset)\n",
    "     \n",
    "\n",
    "    X = dataset.drop('Depression', axis=1, errors='ignore')\n",
    "\n",
    "    if 'Depression' in dataset.columns:\n",
    "        y = dataset['Depression']\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    # setter 0 for missing verdier og så summer sammen A og W pressure i en pressure kolonne\n",
    "    X.fillna({'Academic Pressure': 0}, inplace=True)\n",
    "    X.fillna({'Work Pressure': 0}, inplace=True)\n",
    "\n",
    "    X['Pressure'] = X['Academic Pressure'] + X['Work Pressure']\n",
    "\n",
    "    # dropper \n",
    "    X.drop(['Academic Pressure', 'Work Pressure'], axis=1, inplace=True)\n",
    "    X.drop(['id'],axis=1,inplace=True)\n",
    "\n",
    "    \n",
    "    #name_counts = X['Name'].value_counts().to_dict()\n",
    "    #X['NameCount'] = X['Name'].map(name_counts)\n",
    "\n",
    "\n",
    "    categorical_features = ['Profession', 'Degree','Name','City','Gender', 'Working Professional or Student', 'Sleep Duration', 'Dietary Habits', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']\n",
    "    numerical_features = X.drop(categorical_features, axis=1).columns\n",
    "\n",
    "    if preprocessor is None:\n",
    "       \n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())])\n",
    "        \n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "        \n",
    "        # Kombiner preprocessings.\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numerical_features),\n",
    "                ('cat', categorical_transformer, categorical_features)])\n",
    "        \n",
    "        # Tilpass og transformer X\n",
    "        X_preprocessed = preprocessor.fit_transform(X)\n",
    "    else:\n",
    "        # Transform data basert på eksisterende preprocessor\n",
    "        X_preprocessed = preprocessor.transform(X)\n",
    "\n",
    "    return X_preprocessed, y, preprocessor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size er: 709\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 1891157 stored elements and shape (112560, 709)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t1.1781079788457047\n",
      "  (0, 1)\t-1.3608581883143087e-15\n",
      "  (0, 2)\t2.200024066726753e-15\n",
      "  (0, 3)\t1.601307257907144\n",
      "  (0, 4)\t-0.584280048908395\n",
      "  (0, 5)\t-0.7004473126135766\n",
      "  (0, 6)\t-1.4444817551693707\n",
      "  (0, 58)\t1.0\n",
      "  (0, 84)\t1.0\n",
      "  (0, 444)\t1.0\n",
      "  (0, 564)\t1.0\n",
      "  (0, 648)\t1.0\n",
      "  (0, 650)\t1.0\n",
      "  (0, 679)\t1.0\n",
      "  (0, 702)\t1.0\n",
      "  (0, 705)\t1.0\n",
      "  (0, 708)\t1.0\n",
      "  (1, 0)\t1.0973739427544427\n",
      "  (1, 1)\t-1.3608581883143087e-15\n",
      "  (1, 2)\t2.200024066726753e-15\n",
      "  (1, 3)\t0.8116324006532557\n",
      "  (1, 4)\t1.2319167655524366\n",
      "  (1, 5)\t0.7169565746358945\n",
      "  (1, 6)\t1.4069890684340032\n",
      "  (1, 58)\t1.0\n",
      "  :\t:\n",
      "  (112558, 458)\t1.0\n",
      "  (112558, 627)\t1.0\n",
      "  (112558, 647)\t1.0\n",
      "  (112558, 650)\t1.0\n",
      "  (112558, 669)\t1.0\n",
      "  (112558, 702)\t1.0\n",
      "  (112558, 705)\t1.0\n",
      "  (112558, 707)\t1.0\n",
      "  (112559, 0)\t1.258842014936967\n",
      "  (112559, 1)\t-1.3608581883143087e-15\n",
      "  (112559, 2)\t2.200024066726753e-15\n",
      "  (112559, 3)\t0.021957543399367594\n",
      "  (112559, 4)\t-1.3626501122487513\n",
      "  (112559, 5)\t1.42565851826063\n",
      "  (112559, 6)\t-0.018746343367683844\n",
      "  (112559, 58)\t1.0\n",
      "  (112559, 131)\t1.0\n",
      "  (112559, 514)\t1.0\n",
      "  (112559, 633)\t1.0\n",
      "  (112559, 648)\t1.0\n",
      "  (112559, 650)\t1.0\n",
      "  (112559, 669)\t1.0\n",
      "  (112559, 690)\t1.0\n",
      "  (112559, 705)\t1.0\n",
      "  (112559, 708)\t1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "file_path = \"Data/train.csv\"\n",
    "file_path_test = \"Data/test.csv\"\n",
    "\n",
    "trainingSet = pd.read_csv(file_path)\n",
    "X_test = pd.read_csv(file_path_test)\n",
    "\n",
    "test_ids = X_test['id']\n",
    "  \n",
    "df_train, df_val = train_test_split(trainingSet, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess treningsdata\n",
    "X_train_preprocessed, y_train, preprocessor = prepare_data(df_train)\n",
    "\n",
    "# Preprocess valideringsdata\n",
    "X_val_preprocessed, y_val, _ = prepare_data(df_val, preprocessor=preprocessor)\n",
    "\n",
    "# Preprocess testdata\n",
    "X_test_preprocessed, _, _ = prepare_data(X_test, preprocessor=preprocessor)\n",
    "\n",
    "input_size = X_train_preprocessed.shape[1]\n",
    "print(f'Input size er: {input_size}')\n",
    "print(X_train_preprocessed)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9336176261549396\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators= 100, criterion=\"gini\", max_depth= 40, max_features=\"sqrt\",random_state=42)\n",
    "model.fit(X_train_preprocessed,y_train)\n",
    "\n",
    "y_prediction = model.predict(X_val_preprocessed)\n",
    "\n",
    "\n",
    "print(accuracy_score(y_val,y_prediction))\n",
    "\n",
    "\n",
    "\n",
    "y_test_predict = model.predict(X_test_preprocessed)\n",
    "\n",
    "\n",
    "#output = pd.DataFrame({'id': processed_test[\"id\"], 'Depression': y_test_predict})\n",
    "#output.to_csv('test_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning RF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste parametere: {'criterion': 'entropy', 'max_depth': 40, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "Nøyaktighet på validation settet: 0.9330845771144278\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [20,50,100],\n",
    "    'criterion': ['entropy', 'gini'],\n",
    "    'max_depth': [5, 10, 20 , 40],\n",
    "    'max_features': [\"sqrt\", \"log2\"],\n",
    "}\n",
    "\n",
    "\n",
    "half_grid_search = HalvingGridSearchCV(estimator=model,param_grid=param_grid, factor=3, scoring='accuracy',cv=5)\n",
    "\n",
    "half_grid_search.fit(X_train_preprocessed,y_train)\n",
    "\n",
    "print(f\"Beste parametere: {half_grid_search.best_params_}\")\n",
    "\n",
    "\n",
    "val_accuracy = half_grid_search.score(X_val_preprocessed,y_val)\n",
    "\n",
    "print(f\"Nøyaktighet på validation settet: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 709\n",
    "\n",
    "class DepressionDetector(nn.Module):\n",
    "    #The __init__ method initializes the neural network layers and activation functions.\n",
    "    #nn.Module is initialized using super().__init__() to inherit its functionalities.\n",
    "    def __init__(self):\n",
    "        super().__init__()  \n",
    "        self.layer_1 = nn.Linear(input_size, 64)\n",
    "        self.layer_2 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.layer_1(x))\n",
    "        out = torch.relu(self.layer_2(out))\n",
    "        out = self.sigmoid(self.output(out))\n",
    "        return out    \n",
    "\n",
    "net = DepressionDetector()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konvertere til Tensorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_preprocessed = X_train_preprocessed.toarray() if sparse.issparse(X_train_preprocessed) else X_train_preprocessed\n",
    "X_val_preprocessed = X_val_preprocessed.toarray() if sparse.issparse(X_val_preprocessed) else X_val_preprocessed\n",
    "X_test_preprocessed = X_test_preprocessed.toarray() if sparse.issparse(X_test_preprocessed) else X_test_preprocessed\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_preprocessed, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "X_val_tensor = torch.tensor(X_val_preprocessed, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Lag DataLoaders\n",
    "batch_size = 100\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.07248366624116898\n",
      "Epoch 2/30, Loss: 0.2724033296108246\n",
      "Epoch 3/30, Loss: 0.2520516812801361\n",
      "Epoch 4/30, Loss: 0.13253477215766907\n",
      "Epoch 5/30, Loss: 0.14240612089633942\n",
      "Epoch 6/30, Loss: 0.12472046166658401\n",
      "Epoch 7/30, Loss: 0.05775958672165871\n",
      "Epoch 8/30, Loss: 0.08903398364782333\n",
      "Epoch 9/30, Loss: 0.10771321505308151\n",
      "Epoch 10/30, Loss: 0.17584064602851868\n",
      "Epoch 11/30, Loss: 0.12663890421390533\n",
      "Epoch 12/30, Loss: 0.2351321130990982\n",
      "Epoch 13/30, Loss: 0.13412968814373016\n",
      "Epoch 14/30, Loss: 0.05960649624466896\n",
      "Epoch 15/30, Loss: 0.07568136602640152\n",
      "Epoch 16/30, Loss: 0.12540262937545776\n",
      "Epoch 17/30, Loss: 0.2189619094133377\n",
      "Epoch 18/30, Loss: 0.06711052358150482\n",
      "Epoch 19/30, Loss: 0.06748362630605698\n",
      "Epoch 20/30, Loss: 0.07683656364679337\n",
      "Epoch 21/30, Loss: 0.15260204672813416\n",
      "Epoch 22/30, Loss: 0.05335124209523201\n",
      "Epoch 23/30, Loss: 0.06429307162761688\n",
      "Epoch 24/30, Loss: 0.06694560497999191\n",
      "Epoch 25/30, Loss: 0.1359492987394333\n",
      "Epoch 26/30, Loss: 0.05797705426812172\n",
      "Epoch 27/30, Loss: 0.058597687631845474\n",
      "Epoch 28/30, Loss: 0.07159565389156342\n",
      "Epoch 29/30, Loss: 0.1543617844581604\n",
      "Epoch 30/30, Loss: 0.04332311078906059\n",
      "Valideringssett nøyaktighet: 92.33%\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_tensor.shape[1]\n",
    "model = DepressionDetector()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Treningssløyfe\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # skriv average loss\n",
    "    \n",
    "    # Utskrift av tap etter hver epoch for å overvåke treningen\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Evaluer modellen på valideringssett\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).float()  # Konverter til 0 eller 1 basert på terskelverdien 0.5\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f'Valideringssett nøyaktighet: {accuracy * 100:.2f}%')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    test_predictions = (test_predictions > 0.5).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testprediksjoner er lagret i submission.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output = pd.DataFrame({'id': test_ids, 'Depression': test_predictions.squeeze().int().numpy()})\n",
    "output.to_csv('NN_prediksjoner.csv', index=False)\n",
    "print(\"Testprediksjoner er lagret i submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF367",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
